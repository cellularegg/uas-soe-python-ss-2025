{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45991b37",
   "metadata": {},
   "source": [
    "# Building an Advanced High-Accuracy Movie Recommender\n",
    "\n",
    "Our goal is to construct a movie recommendation system with the highest possible accuracy. This involves several key stages:\n",
    "1.  **Comprehensive Data Loading & Preprocessing:** Ensuring data quality and consistency across all available datasets (movies, ratings, tags, genome scores).\n",
    "2.  **In-depth Feature Engineering:** Creating rich features for users and movies that can capture complex patterns and preferences.\n",
    "3.  **Exploration of Advanced Models:**\n",
    "    *   Advanced Collaborative Filtering (e.g., SVD++, Factorization Machines)\n",
    "    *   Content-Based Filtering (leveraging movie metadata like genres, tags, and potentially descriptions)\n",
    "    *   Knowledge-Based/Graph-Based approaches (using the MovieLens genome tag data)\n",
    "    *   Potentially Deep Learning models (e.g., Neural Collaborative Filtering, Wide & Deep models)\n",
    "4.  **Sophisticated Hybridization:** Combining the strengths of different models using techniques like stacking or feature-weighted blending.\n",
    "5.  **Rigorous Evaluation:** Using appropriate metrics and cross-validation to assess and compare model performance.\n",
    "\n",
    "This notebook will guide us through these stages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7c910d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from surprise import Dataset, Reader, SVD, SVDpp, NMF, KNNBasic\n",
    "from surprise.model_selection import cross_validate, train_test_split as surprise_train_test_split\n",
    "from surprise import accuracy\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "\n",
    "# Display options for pandas\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "print(\"Libraries imported.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4381cbed",
   "metadata": {},
   "source": [
    "## 1. Comprehensive Data Loading & Initial Inspection\n",
    "\n",
    "We will load all relevant Parquet files:\n",
    "*   `movies.parquet`: Movie information (movieId, title, genres).\n",
    "*   `ratings.parquet`: User ratings for movies (userId, movieId, rating, timestamp).\n",
    "*   `tags.parquet`: User-applied tags for movies (userId, movieId, tag, timestamp).\n",
    "*   `genome-tags.parquet`: Genome tag descriptions (tagId, tag).\n",
    "*   `genome-scores.parquet`: Movie-tag relevance scores (movieId, tagId, relevance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "361ac59a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All datasets loaded successfully.\n",
      "Movies: (62423, 3)\n",
      "Ratings: (25000095, 4)\n",
      "Tags: (1093360, 4)\n",
      "Genome Tags: (1128, 2)\n",
      "Genome Scores: (15584448, 3)\n",
      "\\n--- Movies DataFrame Head ---\n",
      "   movieId                               title  \\\n",
      "0        1                    Toy Story (1995)   \n",
      "1        2                      Jumanji (1995)   \n",
      "2        3             Grumpier Old Men (1995)   \n",
      "3        4            Waiting to Exhale (1995)   \n",
      "4        5  Father of the Bride Part II (1995)   \n",
      "\n",
      "                                        genres  \n",
      "0  Adventure|Animation|Children|Comedy|Fantasy  \n",
      "1                   Adventure|Children|Fantasy  \n",
      "2                               Comedy|Romance  \n",
      "3                         Comedy|Drama|Romance  \n",
      "4                                       Comedy  \n",
      "\\n--- Ratings DataFrame Head ---\n",
      "   userId  movieId  rating           timestamp\n",
      "0       1      296     5.0 2006-05-17 15:34:04\n",
      "1       1      306     3.5 2006-05-17 12:26:57\n",
      "2       1      307     5.0 2006-05-17 12:27:08\n",
      "3       1      665     5.0 2006-05-17 15:13:40\n",
      "4       1      899     3.5 2006-05-17 12:21:50\n",
      "\\n--- Tags DataFrame Head ---\n",
      "   userId  movieId               tag           timestamp\n",
      "0       3      260           classic 2015-08-13 13:25:55\n",
      "1       3      260            sci-fi 2015-08-13 13:24:16\n",
      "2       4     1732       dark comedy 2019-11-16 22:33:18\n",
      "3       4     1732    great dialogue 2019-11-16 22:33:24\n",
      "4       4     7569  so bad it's good 2019-11-16 22:30:55\n",
      "\\n--- Genome Tags DataFrame Head ---\n",
      "   tagId           tag\n",
      "0      1           007\n",
      "1      2  007 (series)\n",
      "2      3  18th century\n",
      "3      4         1920s\n",
      "4      5         1930s\n",
      "\\n--- Genome Scores DataFrame Head ---\n",
      "   movieId  tagId  relevance\n",
      "0        1      1    0.02875\n",
      "1        1      2    0.02375\n",
      "2        1      3    0.06250\n",
      "3        1      4    0.07575\n",
      "4        1      5    0.14075\n"
     ]
    }
   ],
   "source": [
    "# Define base path for data\n",
    "base_data_path = '../data/parquet/'\n",
    "\n",
    "# Load datasets\n",
    "try:\n",
    "    movies_df = pd.read_parquet(os.path.join(base_data_path, 'movies.parquet'))\n",
    "    ratings_df = pd.read_parquet(os.path.join(base_data_path, 'ratings.parquet'))\n",
    "    tags_df = pd.read_parquet(os.path.join(base_data_path, 'tags.parquet'))\n",
    "    genome_tags_df = pd.read_parquet(os.path.join(base_data_path, 'genome_tags.parquet'))\n",
    "    genome_scores_df = pd.read_parquet(os.path.join(base_data_path, 'genome_scores.parquet'))\n",
    "\n",
    "    print(\"All datasets loaded successfully.\")\n",
    "    print(f\"Movies: {movies_df.shape}\")\n",
    "    print(f\"Ratings: {ratings_df.shape}\")\n",
    "    print(f\"Tags: {tags_df.shape}\")\n",
    "    print(f\"Genome Tags: {genome_tags_df.shape}\")\n",
    "    print(f\"Genome Scores: {genome_scores_df.shape}\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error loading data: {e}. Please ensure all Parquet files are in {base_data_path}\")\n",
    "\n",
    "# Initial inspection\n",
    "print(\"\\\\n--- Movies DataFrame Head ---\")\n",
    "print(movies_df.head())\n",
    "print(\"\\\\n--- Ratings DataFrame Head ---\")\n",
    "print(ratings_df.head())\n",
    "print(\"\\\\n--- Tags DataFrame Head ---\")\n",
    "print(tags_df.head())\n",
    "print(\"\\\\n--- Genome Tags DataFrame Head ---\")\n",
    "print(genome_tags_df.head())\n",
    "print(\"\\\\n--- Genome Scores DataFrame Head ---\")\n",
    "print(genome_scores_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c5baa2",
   "metadata": {},
   "source": [
    "## 2. Advanced Data Preprocessing and Cleaning\n",
    "\n",
    "This section will focus on cleaning the loaded data and preparing it for feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99c32060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\n--- Processing movies_df ---\n",
      "Movies preprocessing done.\n",
      "                                title    year           title_cleaned  \\\n",
      "0                    Toy Story (1995)  1995.0                ToyStory   \n",
      "1                      Jumanji (1995)  1995.0                 Jumanji   \n",
      "2             Grumpier Old Men (1995)  1995.0          GrumpierOldMen   \n",
      "3            Waiting to Exhale (1995)  1995.0         WaitingtoExhale   \n",
      "4  Father of the Bride Part II (1995)  1995.0  FatheroftheBridePartII   \n",
      "\n",
      "                                         genres_list  \n",
      "0  [Adventure, Animation, Children, Comedy, Fantasy]  \n",
      "1                     [Adventure, Children, Fantasy]  \n",
      "2                                  [Comedy, Romance]  \n",
      "3                           [Comedy, Drama, Romance]  \n",
      "4                                           [Comedy]  \n",
      "\\n--- Processing ratings_df ---\n",
      "Ratings DataFrame processed (timestamp converted to datetime).\n",
      "   userId  movieId  rating           timestamp        timestamp_dt\n",
      "0       1      296     5.0 2006-05-17 15:34:04 2006-05-17 15:34:04\n",
      "1       1      306     3.5 2006-05-17 12:26:57 2006-05-17 12:26:57\n",
      "2       1      307     5.0 2006-05-17 12:27:08 2006-05-17 12:27:08\n",
      "3       1      665     5.0 2006-05-17 15:13:40 2006-05-17 15:13:40\n",
      "4       1      899     3.5 2006-05-17 12:21:50 2006-05-17 12:21:50\n",
      "Movies preprocessing done.\n",
      "                                title    year           title_cleaned  \\\n",
      "0                    Toy Story (1995)  1995.0                ToyStory   \n",
      "1                      Jumanji (1995)  1995.0                 Jumanji   \n",
      "2             Grumpier Old Men (1995)  1995.0          GrumpierOldMen   \n",
      "3            Waiting to Exhale (1995)  1995.0         WaitingtoExhale   \n",
      "4  Father of the Bride Part II (1995)  1995.0  FatheroftheBridePartII   \n",
      "\n",
      "                                         genres_list  \n",
      "0  [Adventure, Animation, Children, Comedy, Fantasy]  \n",
      "1                     [Adventure, Children, Fantasy]  \n",
      "2                                  [Comedy, Romance]  \n",
      "3                           [Comedy, Drama, Romance]  \n",
      "4                                           [Comedy]  \n",
      "\\n--- Processing ratings_df ---\n",
      "Ratings DataFrame processed (timestamp converted to datetime).\n",
      "   userId  movieId  rating           timestamp        timestamp_dt\n",
      "0       1      296     5.0 2006-05-17 15:34:04 2006-05-17 15:34:04\n",
      "1       1      306     3.5 2006-05-17 12:26:57 2006-05-17 12:26:57\n",
      "2       1      307     5.0 2006-05-17 12:27:08 2006-05-17 12:27:08\n",
      "3       1      665     5.0 2006-05-17 15:13:40 2006-05-17 15:13:40\n",
      "4       1      899     3.5 2006-05-17 12:21:50 2006-05-17 12:21:50\n",
      "\\n--- Processing tags_df ---\n",
      "Tags DataFrame processed (timestamp converted, tags cleaned).\n",
      "                tag       tag_cleaned\n",
      "0           classic           classic\n",
      "1            sci-fi            sci-fi\n",
      "2       dark comedy       dark comedy\n",
      "3    great dialogue    great dialogue\n",
      "4  so bad it's good  so bad it's good\n",
      "\\n--- Processing genome_tags_df ---\n",
      "Genome Tags DataFrame processed (tags cleaned).\n",
      "   tagId           tag   tag_cleaned\n",
      "0      1           007           007\n",
      "1      2  007 (series)  007 (series)\n",
      "2      3  18th century  18th century\n",
      "3      4         1920s         1920s\n",
      "4      5         1930s         1930s\n",
      "\\n--- Genome Scores DataFrame Head ---\n",
      "   movieId  tagId  relevance\n",
      "0        1      1    0.02875\n",
      "1        1      2    0.02375\n",
      "2        1      3    0.06250\n",
      "3        1      4    0.07575\n",
      "4        1      5    0.14075\n",
      "\\n--- Missing Values Post-Preprocessing ---\n",
      "Movies: movieId            0\n",
      "title              0\n",
      "genres             0\n",
      "year             412\n",
      "title_cleaned      0\n",
      "genres_list        0\n",
      "dtype: int64\n",
      "\\nRatings: userId          0\n",
      "movieId         0\n",
      "rating          0\n",
      "timestamp       0\n",
      "timestamp_dt    0\n",
      "dtype: int64\n",
      "\\n--- Processing tags_df ---\n",
      "Tags DataFrame processed (timestamp converted, tags cleaned).\n",
      "                tag       tag_cleaned\n",
      "0           classic           classic\n",
      "1            sci-fi            sci-fi\n",
      "2       dark comedy       dark comedy\n",
      "3    great dialogue    great dialogue\n",
      "4  so bad it's good  so bad it's good\n",
      "\\n--- Processing genome_tags_df ---\n",
      "Genome Tags DataFrame processed (tags cleaned).\n",
      "   tagId           tag   tag_cleaned\n",
      "0      1           007           007\n",
      "1      2  007 (series)  007 (series)\n",
      "2      3  18th century  18th century\n",
      "3      4         1920s         1920s\n",
      "4      5         1930s         1930s\n",
      "\\n--- Genome Scores DataFrame Head ---\n",
      "   movieId  tagId  relevance\n",
      "0        1      1    0.02875\n",
      "1        1      2    0.02375\n",
      "2        1      3    0.06250\n",
      "3        1      4    0.07575\n",
      "4        1      5    0.14075\n",
      "\\n--- Missing Values Post-Preprocessing ---\n",
      "Movies: movieId            0\n",
      "title              0\n",
      "genres             0\n",
      "year             412\n",
      "title_cleaned      0\n",
      "genres_list        0\n",
      "dtype: int64\n",
      "\\nRatings: userId          0\n",
      "movieId         0\n",
      "rating          0\n",
      "timestamp       0\n",
      "timestamp_dt    0\n",
      "dtype: int64\n",
      "\\nTags: userId           0\n",
      "movieId          0\n",
      "tag             16\n",
      "timestamp        0\n",
      "timestamp_dt     0\n",
      "tag_cleaned      0\n",
      "dtype: int64\n",
      "\\nGenome Tags: tagId          0\n",
      "tag            0\n",
      "tag_cleaned    0\n",
      "dtype: int64\n",
      "\\nGenome Scores: movieId      0\n",
      "tagId        0\n",
      "relevance    0\n",
      "dtype: int64\n",
      "\n",
      "--- Year column check after preprocessing --- \n",
      "                                title    year           title_cleaned\n",
      "0                    Toy Story (1995)  1995.0                ToyStory\n",
      "1                      Jumanji (1995)  1995.0                 Jumanji\n",
      "2             Grumpier Old Men (1995)  1995.0          GrumpierOldMen\n",
      "3            Waiting to Exhale (1995)  1995.0         WaitingtoExhale\n",
      "4  Father of the Bride Part II (1995)  1995.0  FatheroftheBridePartII\n",
      "Number of NaN years: 412\n",
      "Movies with non-NaN years: 62011\n",
      "Total movies: 62423\n",
      "\n",
      "Movies DataFrame info after preprocessing:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 62423 entries, 0 to 62422\n",
      "Data columns (total 6 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   movieId        62423 non-null  int64  \n",
      " 1   title          62423 non-null  object \n",
      " 2   genres         62423 non-null  object \n",
      " 3   year           62011 non-null  float64\n",
      " 4   title_cleaned  62423 non-null  object \n",
      " 5   genres_list    62423 non-null  object \n",
      "dtypes: float64(1), int64(1), object(4)\n",
      "memory usage: 2.9+ MB\n",
      "\\nTags: userId           0\n",
      "movieId          0\n",
      "tag             16\n",
      "timestamp        0\n",
      "timestamp_dt     0\n",
      "tag_cleaned      0\n",
      "dtype: int64\n",
      "\\nGenome Tags: tagId          0\n",
      "tag            0\n",
      "tag_cleaned    0\n",
      "dtype: int64\n",
      "\\nGenome Scores: movieId      0\n",
      "tagId        0\n",
      "relevance    0\n",
      "dtype: int64\n",
      "\n",
      "--- Year column check after preprocessing --- \n",
      "                                title    year           title_cleaned\n",
      "0                    Toy Story (1995)  1995.0                ToyStory\n",
      "1                      Jumanji (1995)  1995.0                 Jumanji\n",
      "2             Grumpier Old Men (1995)  1995.0          GrumpierOldMen\n",
      "3            Waiting to Exhale (1995)  1995.0         WaitingtoExhale\n",
      "4  Father of the Bride Part II (1995)  1995.0  FatheroftheBridePartII\n",
      "Number of NaN years: 412\n",
      "Movies with non-NaN years: 62011\n",
      "Total movies: 62423\n",
      "\n",
      "Movies DataFrame info after preprocessing:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 62423 entries, 0 to 62422\n",
      "Data columns (total 6 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   movieId        62423 non-null  int64  \n",
      " 1   title          62423 non-null  object \n",
      " 2   genres         62423 non-null  object \n",
      " 3   year           62011 non-null  float64\n",
      " 4   title_cleaned  62423 non-null  object \n",
      " 5   genres_list    62423 non-null  object \n",
      "dtypes: float64(1), int64(1), object(4)\n",
      "memory usage: 2.9+ MB\n"
     ]
    }
   ],
   "source": [
    "# --- Movies DataFrame Preprocessing ---\n",
    "print(\"\\\\n--- Processing movies_df ---\")\n",
    "# Extract year from title\n",
    "movies_df['year'] = movies_df['title'].str.extract(r'\\((\\d{4})\\)\\s*$', expand=False)\n",
    "# Convert year to numeric, coercing errors. This will make non-years NaN\n",
    "movies_df['year'] = pd.to_numeric(movies_df['year'], errors='coerce')\n",
    "\n",
    "# Clean title (remove year and special characters)\n",
    "def clean_title(title):\n",
    "    if not isinstance(title, str):\n",
    "        title = str(title)\n",
    "    # Remove (YYYY) from end, handling potential spaces around it\n",
    "    title_no_year = re.sub(r'\\s*\\(\\d{4}\\)\\s*$', '', title).strip()\n",
    "    # If the year was not at the very end, it might still be in the title_no_year\n",
    "    # This regex is more general for removing (YYYY) if it's embedded or for titles that are just (YYYY)\n",
    "    title_no_year = re.sub(r'\\((\\d{4})\\)', '', title_no_year).strip()\n",
    "    # Keep basic punctuation useful for titles, remove others\n",
    "    # Using double quotes for the raw string and hyphen at the end of the character class.\n",
    "    title_cleaned = re.sub(r\"[^a-zA-Z0-9\\\\\\\\s:,&.'-]\", '', title_no_year)\n",
    "    title_cleaned = title_cleaned.strip()\n",
    "    # If cleaning results in an empty string (e.g. title was only special chars or just '(YYYY)'),\n",
    "    # revert to a minimally cleaned version of the original title to avoid empty titles.\n",
    "    if not title_cleaned and title_no_year: # If cleaning removed everything but original had content\n",
    "        title_cleaned = re.sub(r\"[^a-zA-Z0-9\\\\\\\\s]\", '', title_no_year).strip() # a more permissive cleaning\n",
    "    if not title_cleaned: # If still empty, use original title but remove (YYYY)\n",
    "        title_cleaned = re.sub(r'\\s*\\(\\d{4}\\)\\s*$', '', title).strip()\n",
    "        title_cleaned = re.sub(r'\\((\\d{4})\\)', '', title_cleaned).strip()\n",
    "        if not title_cleaned: # If original title was just (YYYY) or similar\n",
    "             title_cleaned = title # Fallback to original if all else fails to produce a string\n",
    "    return title_cleaned\n",
    "\n",
    "movies_df['title_cleaned'] = movies_df['title'].apply(clean_title)\n",
    "\n",
    "# Split genres string into a list of genres\n",
    "movies_df['genres_list'] = movies_df['genres'].apply(lambda x: x.split('|') if isinstance(x, str) else [])\n",
    "print(\"Movies preprocessing done.\")\n",
    "print(movies_df[['title', 'year', 'title_cleaned', 'genres_list']].head())\n",
    "\n",
    "# --- Ratings DataFrame Preprocessing ---\n",
    "# Convert timestamp to datetime (optional, but useful for time-aware features)\n",
    "ratings_df['timestamp_dt'] = pd.to_datetime(ratings_df['timestamp'], unit='s')\n",
    "print(\"\\\\n--- Processing ratings_df ---\")\n",
    "print(\"Ratings DataFrame processed (timestamp converted to datetime).\")\n",
    "print(ratings_df.head())\n",
    "\n",
    "\n",
    "# --- Tags DataFrame Preprocessing ---\n",
    "# Convert timestamp to datetime\n",
    "tags_df['timestamp_dt'] = pd.to_datetime(tags_df['timestamp'], unit='s')\n",
    "# Lowercase and strip whitespace from tags\n",
    "tags_df['tag_cleaned'] = tags_df['tag'].astype(str).str.lower().str.strip()\n",
    "print(\"\\\\n--- Processing tags_df ---\")\n",
    "print(\"Tags DataFrame processed (timestamp converted, tags cleaned).\")\n",
    "print(tags_df[['tag', 'tag_cleaned']].head())\n",
    "\n",
    "# --- Genome Tags DataFrame Preprocessing ---\n",
    "# Lowercase and strip whitespace from genome tags\n",
    "genome_tags_df['tag_cleaned'] = genome_tags_df['tag'].astype(str).str.lower().str.strip()\n",
    "print(\"\\\\n--- Processing genome_tags_df ---\")\n",
    "print(\"Genome Tags DataFrame processed (tags cleaned).\")\n",
    "print(genome_tags_df.head())\n",
    "\n",
    "# --- Genome Scores DataFrame Preprocessing ---\n",
    "# No immediate cleaning needed for genome_scores, but we'll merge it later.\n",
    "print(\"\\\\n--- Genome Scores DataFrame Head ---\")\n",
    "print(genome_scores_df.head())\n",
    "\n",
    "\n",
    "# Check for missing values after initial processing\n",
    "print(\"\\\\n--- Missing Values Post-Preprocessing ---\")\n",
    "print(\"Movies:\", movies_df.isnull().sum())\n",
    "print(\"\\\\nRatings:\", ratings_df.isnull().sum())\n",
    "print(\"\\\\nTags:\", tags_df.isnull().sum()) # tag can have NaNs if original had them\n",
    "print(\"\\\\nGenome Tags:\", genome_tags_df.isnull().sum())\n",
    "print(\"\\\\nGenome Scores:\", genome_scores_df.isnull().sum())\n",
    "\n",
    "print(\"\\n--- Year column check after preprocessing --- \")\n",
    "print(movies_df[['title', 'year', 'title_cleaned']].head())\n",
    "print(f\"Number of NaN years: {movies_df['year'].isnull().sum()}\")\n",
    "print(f\"Movies with non-NaN years: {movies_df['year'].notnull().sum()}\")\n",
    "print(f\"Total movies: {len(movies_df)}\")\n",
    "print(\"\\nMovies DataFrame info after preprocessing:\")\n",
    "movies_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95cbfe74",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering\n",
    "\n",
    "Now, we'll create features that will be used by our recommendation models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c94d322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\n--- Engineering Movie Features ---\n",
      "Movie features (avg_rating, rating_count) engineered.\n",
      "   movieId           title_cleaned  movie_avg_rating  movie_rating_count\n",
      "0        1                ToyStory          3.893708             57309.0\n",
      "1        2                 Jumanji          3.251527             24228.0\n",
      "2        3          GrumpierOldMen          3.142028             11804.0\n",
      "3        4         WaitingtoExhale          2.853547              2523.0\n",
      "4        5  FatheroftheBridePartII          3.058434             11714.0\n",
      "\\n--- Engineering User Features ---\n",
      "Movie features (avg_rating, rating_count) engineered.\n",
      "   movieId           title_cleaned  movie_avg_rating  movie_rating_count\n",
      "0        1                ToyStory          3.893708             57309.0\n",
      "1        2                 Jumanji          3.251527             24228.0\n",
      "2        3          GrumpierOldMen          3.142028             11804.0\n",
      "3        4         WaitingtoExhale          2.853547              2523.0\n",
      "4        5  FatheroftheBridePartII          3.058434             11714.0\n",
      "\\n--- Engineering User Features ---\n",
      "User features (avg_rating, rating_count) engineered.\n",
      "        user_avg_rating  user_rating_count\n",
      "userId                                    \n",
      "1              3.814286                 70\n",
      "2              3.630435                184\n",
      "3              3.697409                656\n",
      "4              3.378099                242\n",
      "5              3.752475                101\n",
      "\\n--- Engineering Movie Tag Profiles (from Genome Scores) ---\n",
      "User features (avg_rating, rating_count) engineered.\n",
      "        user_avg_rating  user_rating_count\n",
      "userId                                    \n",
      "1              3.814286                 70\n",
      "2              3.630435                184\n",
      "3              3.697409                656\n",
      "4              3.378099                242\n",
      "5              3.752475                101\n",
      "\\n--- Engineering Movie Tag Profiles (from Genome Scores) ---\n",
      "Movie genome tag profiles engineered.\n",
      "   movieId           title_cleaned  \\\n",
      "0        1                ToyStory   \n",
      "1        2                 Jumanji   \n",
      "2        3          GrumpierOldMen   \n",
      "3        4         WaitingtoExhale   \n",
      "4        5  FatheroftheBridePartII   \n",
      "\n",
      "                                                                                                                                                                                                                                                                                      genome_tag_profile  \n",
      "0  adventure animated animation cartoon cgi childhood children classic computer animation cute disney disney animated feature family friendship fun good great great movie heartwarming imdb top 250 kids kids and family light nostalgic original pixar pixar animation story toys unlikely friendships  \n",
      "1                                                                                                                                                                          adventure animals based on a book childhood children family fantasy fantasy world fun movie jungle kids lions special effects  \n",
      "2                                                                                                                                                                                                                                                                      comedy good sequel sequel sequels  \n",
      "3                                                                                                                                                                                                                                                                              chick flick divorce women  \n",
      "4                                                                                                                                                                                                         comedy family father daughter relationship good sequel midlife crisis pregnancy sequel sequels  \n",
      "\n",
      "Movies DataFrame shape: (62423, 9)\n",
      "User DataFrame shape: (162541, 2)\n",
      "Ratings DataFrame shape: (25000095, 5)\n",
      "\n",
      "NaNs in movies_df after feature engineering:\n",
      "movieId                 0\n",
      "title                   0\n",
      "genres                  0\n",
      "year                  412\n",
      "title_cleaned           0\n",
      "genres_list             0\n",
      "movie_avg_rating        0\n",
      "movie_rating_count      0\n",
      "genome_tag_profile      0\n",
      "dtype: int64\n",
      "\n",
      "NaNs in user_df after feature engineering:\n",
      "user_avg_rating      0\n",
      "user_rating_count    0\n",
      "dtype: int64\n",
      "Movie genome tag profiles engineered.\n",
      "   movieId           title_cleaned  \\\n",
      "0        1                ToyStory   \n",
      "1        2                 Jumanji   \n",
      "2        3          GrumpierOldMen   \n",
      "3        4         WaitingtoExhale   \n",
      "4        5  FatheroftheBridePartII   \n",
      "\n",
      "                                                                                                                                                                                                                                                                                      genome_tag_profile  \n",
      "0  adventure animated animation cartoon cgi childhood children classic computer animation cute disney disney animated feature family friendship fun good great great movie heartwarming imdb top 250 kids kids and family light nostalgic original pixar pixar animation story toys unlikely friendships  \n",
      "1                                                                                                                                                                          adventure animals based on a book childhood children family fantasy fantasy world fun movie jungle kids lions special effects  \n",
      "2                                                                                                                                                                                                                                                                      comedy good sequel sequel sequels  \n",
      "3                                                                                                                                                                                                                                                                              chick flick divorce women  \n",
      "4                                                                                                                                                                                                         comedy family father daughter relationship good sequel midlife crisis pregnancy sequel sequels  \n",
      "\n",
      "Movies DataFrame shape: (62423, 9)\n",
      "User DataFrame shape: (162541, 2)\n",
      "Ratings DataFrame shape: (25000095, 5)\n",
      "\n",
      "NaNs in movies_df after feature engineering:\n",
      "movieId                 0\n",
      "title                   0\n",
      "genres                  0\n",
      "year                  412\n",
      "title_cleaned           0\n",
      "genres_list             0\n",
      "movie_avg_rating        0\n",
      "movie_rating_count      0\n",
      "genome_tag_profile      0\n",
      "dtype: int64\n",
      "\n",
      "NaNs in user_df after feature engineering:\n",
      "user_avg_rating      0\n",
      "user_rating_count    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# --- Movie Features ---\n",
    "print(\"\\\\n--- Engineering Movie Features ---\")\n",
    "# Clean up columns from previous erroneous runs if they exist\n",
    "# This is to prevent issues if the notebook is re-run in a state where these columns were already created incorrectly.\n",
    "columns_to_drop = [\n",
    "    'movie_avg_rating_x', 'movie_rating_count_x',\n",
    "    'movie_avg_rating_y', 'movie_rating_count_y',\n",
    "    'movie_avg_rating', 'movie_rating_count', # Drop these too, to ensure they are recreated correctly\n",
    "    'genome_tag_profile' # Also drop and recreate this\n",
    "]\n",
    "for col in columns_to_drop:\n",
    "    if col in movies_df.columns:\n",
    "        movies_df.drop(columns=[col], inplace=True)\n",
    "\n",
    "# Calculate average ratings and rating counts per movie\n",
    "movie_avg_ratings_series = ratings_df.groupby('movieId')['rating'].mean().rename('movie_avg_rating')\n",
    "movie_rating_counts_series = ratings_df.groupby('movieId')['rating'].count().rename('movie_rating_count')\n",
    "\n",
    "# Convert series to dataframes before merging\n",
    "movie_avg_ratings_df = movie_avg_ratings_series.to_frame() # movieId is index\n",
    "movie_rating_counts_df = movie_rating_counts_series.to_frame() # movieId is index\n",
    "\n",
    "# Merge these features back into movies_df\n",
    "# movies_df has 'movieId' as a column. The new DFs have 'movieId' as index.\n",
    "movies_df = movies_df.merge(movie_avg_ratings_df, left_on='movieId', right_index=True, how='left')\n",
    "movies_df = movies_df.merge(movie_rating_counts_df, left_on='movieId', right_index=True, how='left')\n",
    "\n",
    "# Fill NaN for movies with no ratings (if any)\n",
    "movies_df['movie_avg_rating'] = movies_df['movie_avg_rating'].fillna(0) # Or use global mean rating\n",
    "movies_df['movie_rating_count'] = movies_df['movie_rating_count'].fillna(0)\n",
    "\n",
    "print(\"Movie features (avg_rating, rating_count) engineered.\")\n",
    "print(movies_df[['movieId', 'title_cleaned', 'movie_avg_rating', 'movie_rating_count']].head())\n",
    "\n",
    "# --- User Features ---\n",
    "print(\"\\\\n--- Engineering User Features ---\")\n",
    "user_avg_ratings = ratings_df.groupby('userId')['rating'].mean().rename('user_avg_rating')\n",
    "user_rating_counts = ratings_df.groupby('userId')['rating'].count().rename('user_rating_count')\n",
    "\n",
    "# Create a user_df. userId will be the index.\n",
    "user_df = pd.DataFrame(index=ratings_df['userId'].unique())\n",
    "user_df.index.name = 'userId'\n",
    "user_df = user_df.merge(user_avg_ratings, left_index=True, right_index=True, how='left')\n",
    "user_df = user_df.merge(user_rating_counts, left_index=True, right_index=True, how='left')\n",
    "\n",
    "# Fill NaN for users (if any unusual cases, though groupby should cover all users in ratings_df)\n",
    "user_df['user_avg_rating'] = user_df['user_avg_rating'].fillna(user_df['user_avg_rating'].mean()) # Or 0\n",
    "user_df['user_rating_count'] = user_df['user_rating_count'].fillna(0)\n",
    "\n",
    "\n",
    "print(\"User features (avg_rating, rating_count) engineered.\")\n",
    "print(user_df.head())\n",
    "# Example: To add user features to ratings_df (if a model needs it in this flat format)\n",
    "# ratings_df = ratings_df.merge(user_df, on='userId', how='left')\n",
    "\n",
    "\n",
    "# --- Merging Genome Tags with Scores for Movie Tag Profiles ---\n",
    "print(\"\\\\n--- Engineering Movie Tag Profiles (from Genome Scores) ---\")\n",
    "# Merge genome_scores with genome_tags to get tag names\n",
    "# Ensure genome_scores_df and genome_tags_df are in their original state if this cell is re-run\n",
    "# This merge should ideally happen once, or the DFs should be copies if modified in place.\n",
    "# For simplicity, assuming they are loaded fresh or not modified in a way that affects this merge.\n",
    "if 'tag' not in genome_scores_df.columns and 'tagId' in genome_scores_df.columns and 'tagId' in genome_tags_df.columns:\n",
    "    genome_scores_df_merged = genome_scores_df.merge(genome_tags_df, on='tagId', how='left')\n",
    "else: # if already merged or structure is different, assume it's ready or use a copy\n",
    "    genome_scores_df_merged = genome_scores_df.copy() \n",
    "\n",
    "# Ensure 'tag_cleaned' is present or created\n",
    "if 'tag_cleaned' not in genome_scores_df_merged.columns and 'tag' in genome_scores_df_merged.columns:\n",
    "    genome_scores_df_merged['tag_cleaned'] = genome_scores_df_merged['tag'].apply(lambda x: re.sub(r'[^a-zA-Z0-9_\\s]', '', str(x).lower()))\n",
    "elif 'tag_cleaned' not in genome_scores_df_merged.columns: # Fallback if 'tag' is also missing\n",
    "    genome_scores_df_merged['tag_cleaned'] = \"\" # Add empty column to prevent error\n",
    "\n",
    "\n",
    "MIN_RELEVANCE = 0.8\n",
    "# Check if 'movieId' and 'tag_cleaned' are in the dataframe to prevent KeyError\n",
    "if 'movieId' in genome_scores_df_merged.columns and 'tag_cleaned' in genome_scores_df_merged.columns and 'relevance' in genome_scores_df_merged.columns:\n",
    "    movie_genome_tag_profiles_series = genome_scores_df_merged[genome_scores_df_merged['relevance'] > MIN_RELEVANCE].groupby('movieId')['tag_cleaned'].apply(lambda tags: ' '.join(sorted(list(tags)))).rename('genome_tag_profile')\n",
    "    # Convert series to dataframe before merging\n",
    "    movie_genome_tag_profiles_df = movie_genome_tag_profiles_series.to_frame() # movieId is index\n",
    "    movies_df = movies_df.merge(movie_genome_tag_profiles_df, left_on='movieId', right_index=True, how='left')\n",
    "else:\n",
    "    print(\"Skipping genome tag profile generation due to missing columns in genome_scores_df_merged.\")\n",
    "    # Ensure the column exists even if not populated, to prevent downstream errors\n",
    "    if 'genome_tag_profile' not in movies_df.columns:\n",
    "         movies_df['genome_tag_profile'] = '' \n",
    "\n",
    "movies_df['genome_tag_profile'] = movies_df['genome_tag_profile'].fillna('') # Fill NaN for movies with no significant genome tags\n",
    "\n",
    "print(\"Movie genome tag profiles engineered.\")\n",
    "print(movies_df[['movieId', 'title_cleaned', 'genome_tag_profile']].head())\n",
    "\n",
    "# Display some basic stats or info\n",
    "print(f\"\\nMovies DataFrame shape: {movies_df.shape}\")\n",
    "print(f\"User DataFrame shape: {user_df.shape}\")\n",
    "print(f\"Ratings DataFrame shape: {ratings_df.shape}\") # ratings_df is not modified here yet\n",
    "\n",
    "# Check for NaNs created by merges, if any unexpected\n",
    "print(\"\\nNaNs in movies_df after feature engineering:\")\n",
    "print(movies_df.isnull().sum())\n",
    "print(\"\\nNaNs in user_df after feature engineering:\")\n",
    "print(user_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010aea61",
   "metadata": {},
   "source": [
    "## 4. Model Building - Part 1: Advanced Collaborative Filtering\n",
    "\n",
    "We'll start with advanced matrix factorization techniques like SVD++ or NMF.\n",
    "We will use the `surprise` library.\n",
    "\n",
    "**Note on Parallelism:** The `surprise` library, while convenient, does not inherently parallelize the training of all its algorithms (like SVD, SVD++, NMF) across multiple CPU cores in the same way some other libraries (e.g., scikit-learn with `n_jobs`) do. For very large datasets or computationally intensive hyperparameter searches, training times might be significant. If performance becomes a major bottleneck for these CF models, alternatives like LightFM, Implicit, xLearn, or custom implementations in TensorFlow/PyTorch (which offer better control over parallelism) should be considered. For now, we will use `surprise` to establish baseline performance for these standard CF algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01e4f991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainset/Testset pickle files not found. Creating and saving them...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving trainset_surprise to ../models/trainset_surprise.pkl...\n",
      "Saving testset_surprise to ../models/testset_surprise.pkl...\n",
      "Saving testset_surprise to ../models/testset_surprise.pkl...\n",
      "Trainset and Testset created and saved to files.\n",
      "\\n--- Training SVD ---\n",
      "Trainset and Testset created and saved to files.\n",
      "\\n--- Training SVD ---\n",
      "RMSE: 0.7773\n",
      "SVD RMSE: 0.7772926722924584\n",
      "\n",
      "--- Training SVD++ ---\n",
      "RMSE: 0.7773\n",
      "SVD RMSE: 0.7772926722924584\n",
      "\n",
      "--- Training SVD++ ---\n",
      "RMSE: 0.8263\n",
      "SVD++ RMSE: 0.8262693807476258\n",
      "\n",
      "--- Training NMF ---\n",
      "RMSE: 0.8263\n",
      "SVD++ RMSE: 0.8262693807476258\n",
      "\n",
      "--- Training NMF ---\n",
      "RMSE: 0.8865\n",
      "NMF RMSE: 0.8864854144950411\n",
      "RMSE: 0.8865\n",
      "NMF RMSE: 0.8864854144950411\n"
     ]
    }
   ],
   "source": [
    "# Prepare data for Surprise\n",
    "reader = Reader(rating_scale=(0.5, 5.0))\n",
    "# IMPORTANT: The ML-25M dataset has 25 million ratings.\n",
    "# Training Surprise models (especially SVD, SVD++, NMF) on the full dataset can be VERY time-consuming\n",
    "# and memory-intensive. For initial development and iteration, consider sampling the ratings_df.\n",
    "# Example: ratings_df_sample = ratings_df.sample(n=1000000, random_state=42)\n",
    "# Then use ratings_df_sample in Dataset.load_from_df()\n",
    "# For this run, we will proceed with the full dataset, be prepared for potential long runtimes.\n",
    "\n",
    "# Define paths for saving/loading trainset and testset\n",
    "models_dir = \"../models/\"\n",
    "if not os.path.exists(models_dir):\n",
    "    os.makedirs(models_dir)\n",
    "path_trainset_surprise = os.path.join(models_dir, \"trainset_surprise.pkl\")\n",
    "path_testset_surprise = os.path.join(models_dir, \"testset_surprise.pkl\")\n",
    "\n",
    "# Attempt to load trainset and testset from files\n",
    "if os.path.exists(path_trainset_surprise) and os.path.exists(path_testset_surprise):\n",
    "    print(f\"Loading trainset_surprise from {path_trainset_surprise}...\")\n",
    "    with open(path_trainset_surprise, 'rb') as f:\n",
    "        trainset_surprise = pickle.load(f)\n",
    "    print(f\"Loading testset_surprise from {path_testset_surprise}...\")\n",
    "    with open(path_testset_surprise, 'rb') as f:\n",
    "        testset_surprise = pickle.load(f)\n",
    "    print(\"Trainset and Testset loaded from files.\")\n",
    "    # Rebuild the full data_surprise object if needed for other operations,\n",
    "    # or ensure downstream tasks only need trainset/testset.\n",
    "    # For now, we assume trainset and testset are sufficient.\n",
    "    # If data_surprise is needed elsewhere, it might need to be reconstructed or also saved/loaded.\n",
    "else:\n",
    "    print(\"Trainset/Testset pickle files not found. Creating and saving them...\")\n",
    "    data_surprise = Dataset.load_from_df(ratings_df[['userId', 'movieId', 'rating']], reader)\n",
    "    trainset_surprise, testset_surprise = surprise_train_test_split(data_surprise, test_size=0.2, random_state=42)\n",
    "    \n",
    "    print(f\"Saving trainset_surprise to {path_trainset_surprise}...\")\n",
    "    with open(path_trainset_surprise, 'wb') as f:\n",
    "        pickle.dump(trainset_surprise, f)\n",
    "    print(f\"Saving testset_surprise to {path_testset_surprise}...\")\n",
    "    with open(path_testset_surprise, 'wb') as f:\n",
    "        pickle.dump(testset_surprise, f)\n",
    "    print(\"Trainset and Testset created and saved to files.\")\n",
    "\n",
    "print(\"\\\\n--- Training SVD ---\")\n",
    "algo_svd = SVD(n_factors=100, n_epochs=20, random_state=42, verbose=False) # Basic SVD\n",
    "# cross_validate(algo_svd, data_surprise, measures=['RMSE', 'MAE'], cv=3, verbose=True) # CV is time-consuming\n",
    "algo_svd.fit(trainset_surprise)\n",
    "predictions_svd = algo_svd.test(testset_surprise)\n",
    "rmse_svd = accuracy.rmse(predictions_svd)\n",
    "print(f\"SVD RMSE: {rmse_svd}\")\n",
    "\n",
    "print(\"\\n--- Training SVD++ ---\")\n",
    "# SVD++ considers implicit feedback. It needs user-item interactions.\n",
    "# The current ratings_df is explicit. For SVD++ to use its full potential,\n",
    "# we might need to represent all rated items by a user as implicit feedback.\n",
    "# However, Surprise's SVD++ can run on explicit ratings too.\n",
    "algo_svdpp = SVDpp(n_factors=50, n_epochs=10, random_state=42, verbose=False, cache_ratings=True) # Fewer epochs due to time\n",
    "# cross_validate(algo_svdpp, data_surprise, measures=['RMSE', 'MAE'], cv=3, verbose=True) # Very time-consuming\n",
    "algo_svdpp.fit(trainset_surprise)\n",
    "predictions_svdpp = algo_svdpp.test(testset_surprise)\n",
    "rmse_svdpp = accuracy.rmse(predictions_svdpp)\n",
    "print(f\"SVD++ RMSE: {rmse_svdpp}\")\n",
    "\n",
    "\n",
    "print(\"\\n--- Training NMF ---\")\n",
    "algo_nmf = NMF(n_factors=15, n_epochs=20, random_state=42, verbose=False) # NMF\n",
    "# cross_validate(algo_nmf, data_surprise, measures=['RMSE', 'MAE'], cv=3, verbose=True)\n",
    "algo_nmf.fit(trainset_surprise)\n",
    "predictions_nmf = algo_nmf.test(testset_surprise)\n",
    "rmse_nmf = accuracy.rmse(predictions_nmf)\n",
    "print(f\"NMF RMSE: {rmse_nmf}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed8d705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporary cell to check if CF models are in memory\n",
    "cf_models_in_memory = True\n",
    "if 'algo_svd' not in locals() or 'algo_svdpp' not in locals() or 'algo_nmf' not in locals():\n",
    "    cf_models_in_memory = False\n",
    "print(f\"Collaborative filtering models in memory: {cf_models_in_memory}\")\n",
    "\n",
    "if not cf_models_in_memory:\n",
    "    print(\"Attempting to re-run CF training cell...\")\n",
    "    # This is a placeholder, actual re-run will be a separate call if needed\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e7e3ed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Saving Collaborative Filtering Models ---\n",
      "SVD model (algo_svd) not found in memory, skipping save.\n",
      "SVD++ model (algo_svdpp) not found in memory, skipping save.\n",
      "NMF model (algo_nmf) not found in memory, skipping save.\n",
      "--- Collaborative Filtering Models processed for saving. ---\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Saving Collaborative Filtering Models ---\")\n",
    "models_dir = \"../models/\"\n",
    "if not os.path.exists(models_dir):\n",
    "    os.makedirs(models_dir)\n",
    "    print(f\"Created directory: {models_dir}\")\n",
    "\n",
    "path_svd_model = os.path.join(models_dir, \"adv_algo_svd.pkl\")\n",
    "path_svdpp_model = os.path.join(models_dir, \"adv_algo_svdpp.pkl\")\n",
    "path_nmf_model = os.path.join(models_dir, \"adv_algo_nmf.pkl\")\n",
    "\n",
    "# Save Surprise models\n",
    "if 'algo_svd' in locals():\n",
    "    with open(path_svd_model, 'wb') as f:\n",
    "        pickle.dump(algo_svd, f)\n",
    "    print(f\"Saved SVD model to {path_svd_model}\")\n",
    "else:\n",
    "    print(\"SVD model (algo_svd) not found in memory, skipping save.\")\n",
    "\n",
    "if 'algo_svdpp' in locals():\n",
    "    with open(path_svdpp_model, 'wb') as f:\n",
    "        pickle.dump(algo_svdpp, f)\n",
    "    print(f\"Saved SVD++ model to {path_svdpp_model}\")\n",
    "else:\n",
    "    print(\"SVD++ model (algo_svdpp) not found in memory, skipping save.\")\n",
    "\n",
    "if 'algo_nmf' in locals():\n",
    "    with open(path_nmf_model, 'wb') as f:\n",
    "        pickle.dump(algo_nmf, f)\n",
    "    print(f\"Saved NMF model to {path_nmf_model}\")\n",
    "else:\n",
    "    print(\"NMF model (algo_nmf) not found in memory, skipping save.\")\n",
    "print(\"--- Collaborative Filtering Models processed for saving. ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa33a8b",
   "metadata": {},
   "source": [
    "## 5. Model Building - Part 2: Content-Based Filtering\n",
    "\n",
    "Here, we'll build a content-based recommender using movie genres and genome tag profiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f73e9016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\\\n--- Building Content-Based Model ---\n",
      "TF-IDF matrix shape: (62423, 1072)\n",
      "TF-IDF matrix shape: (62423, 1072)\n",
      "Content cosine similarity matrix shape: (62423, 62423)\n",
      "Content movie indices lookup created.\n",
      "\\\\nExample Content-Based Recommendations for movie: ToyStory (ID: 1)\n",
      "       movieId  title_cleaned  \\\n",
      "3021      3114      ToyStory2   \n",
      "4780      4886  Monsters,Inc.   \n",
      "2264      2355    Bug'sLife,A   \n",
      "14813    78499      ToyStory3   \n",
      "11361    50872    Ratatouille   \n",
      "\n",
      "                                                   genres_list  \\\n",
      "3021         [Adventure, Animation, Children, Comedy, Fantasy]   \n",
      "4780         [Adventure, Animation, Children, Comedy, Fantasy]   \n",
      "2264                  [Adventure, Animation, Children, Comedy]   \n",
      "14813  [Adventure, Animation, Children, Comedy, Fantasy, IMAX]   \n",
      "11361                             [Animation, Children, Drama]   \n",
      "\n",
      "       similarity_score  \n",
      "3021           0.847621  \n",
      "4780           0.808108  \n",
      "2264           0.796815  \n",
      "14813          0.778656  \n",
      "11361          0.701561  \n",
      "Content cosine similarity matrix shape: (62423, 62423)\n",
      "Content movie indices lookup created.\n",
      "\\\\nExample Content-Based Recommendations for movie: ToyStory (ID: 1)\n",
      "       movieId  title_cleaned  \\\n",
      "3021      3114      ToyStory2   \n",
      "4780      4886  Monsters,Inc.   \n",
      "2264      2355    Bug'sLife,A   \n",
      "14813    78499      ToyStory3   \n",
      "11361    50872    Ratatouille   \n",
      "\n",
      "                                                   genres_list  \\\n",
      "3021         [Adventure, Animation, Children, Comedy, Fantasy]   \n",
      "4780         [Adventure, Animation, Children, Comedy, Fantasy]   \n",
      "2264                  [Adventure, Animation, Children, Comedy]   \n",
      "14813  [Adventure, Animation, Children, Comedy, Fantasy, IMAX]   \n",
      "11361                             [Animation, Children, Drama]   \n",
      "\n",
      "       similarity_score  \n",
      "3021           0.847621  \n",
      "4780           0.808108  \n",
      "2264           0.796815  \n",
      "14813          0.778656  \n",
      "11361          0.701561  \n"
     ]
    }
   ],
   "source": [
    "print(\"\\\\\\\\n--- Building Content-Based Model ---\")\n",
    "\n",
    "# Combine genres and genome tag profile for a richer content representation\n",
    "movies_df['content_features_str'] = movies_df['genres_list'].apply(lambda x: ' '.join(x)) + ' ' + movies_df['genome_tag_profile']\n",
    "\n",
    "# Use TF-IDF to vectorize these content features\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', min_df=5) # min_df to ignore very rare terms\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(movies_df['content_features_str'])\n",
    "\n",
    "print(f\"TF-IDF matrix shape: {tfidf_matrix.shape}\")\n",
    "\n",
    "# Compute cosine similarity matrix from TF-IDF features\n",
    "# Note: For a large number of movies (e.g., 62k in ML-25M), this matrix (num_movies x num_movies)\n",
    "# can be very large (e.g., ~15GB for float32). Consider alternatives like approximate nearest neighbors\n",
    "# (e.g., Annoy, Faiss, ScaNN) for TF-IDF vectors if memory or computation speed becomes an issue.\n",
    "cosine_sim_content = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "print(f\"Content cosine similarity matrix shape: {cosine_sim_content.shape}\")\n",
    "\n",
    "# Create a lookup for movie titles to their index in the cosine_sim_content matrix\n",
    "# This assumes titles are unique enough to serve as keys.\n",
    "# If titles are not unique, this strategy needs revision (e.g., use movieId).\n",
    "# The indices of cosine_sim_content correspond to the rows of movies_df at the time of TF-IDF fitting.\n",
    "content_movie_indices_lookup = pd.Series(range(cosine_sim_content.shape[0]), index=movies_df['title'])\n",
    "print(\"Content movie indices lookup created.\")\n",
    "\n",
    "# Function to get content-based recommendations\n",
    "# This provides item-item similarity. To get user recommendations:\n",
    "# 1. Find movies highly rated by the user.\n",
    "# 2. For each of these movies, find the most similar movies using cosine_sim_content.\n",
    "# 3. Aggregate these similar movies and rank them.\n",
    "\n",
    "def get_content_recommendations(movie_id, num_recs=10):\n",
    "    if movie_id not in movies_df['movieId'].values:\n",
    "        print(f\"MovieId {movie_id} not found.\")\n",
    "        return []\n",
    "    \n",
    "    movie_idx = movies_df[movies_df['movieId'] == movie_id].index[0]\n",
    "    sim_scores = list(enumerate(cosine_sim_content[movie_idx]))\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "    sim_scores = sim_scores[1:num_recs+1] # Exclude the movie itself\n",
    "    \n",
    "    recommended_movie_indices = [i[0] for i in sim_scores]\n",
    "    recommended_movies = movies_df.iloc[recommended_movie_indices][['movieId', 'title_cleaned', 'genres_list']]\n",
    "    recommended_movies['similarity_score'] = [s[1] for s in sim_scores]\n",
    "    return recommended_movies\n",
    "\n",
    "# Example: Get recommendations for a movie\n",
    "if not movies_df.empty:\n",
    "    example_movie_id_for_content = movies_df['movieId'].iloc[0]\n",
    "    print(f\"\\\\\\\\nExample Content-Based Recommendations for movie: {movies_df[movies_df['movieId'] == example_movie_id_for_content]['title_cleaned'].values[0]} (ID: {example_movie_id_for_content})\")\n",
    "    content_recs_example = get_content_recommendations(example_movie_id_for_content, 5)\n",
    "    print(content_recs_example)\n",
    "else:\n",
    "    print(\"movies_df is empty, cannot run content-based recommendation example.\")\n",
    "\n",
    "# Note: Evaluating content-based models in terms of rating prediction RMSE is not direct.\n",
    "# They are typically evaluated on precision/recall of recommended items or diversity.\n",
    "# For a hybrid model, we might use the similarity scores or predicted ratings derived from similar items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fe9e10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Saving Content-Based Components ---\n",
      "Saved TF-IDF vectorizer to ../models/adv_tfidf_vectorizer.pkl\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Saving Content-Based Components ---\")\n",
    "models_dir = \"../models/\"\n",
    "if not os.path.exists(models_dir):\n",
    "    os.makedirs(models_dir)\n",
    "    print(f\"Created directory: {models_dir}\")\n",
    "\n",
    "path_tfidf_vectorizer = os.path.join(models_dir, \"adv_tfidf_vectorizer.pkl\")\n",
    "path_cosine_sim_content = os.path.join(models_dir, \"adv_cosine_sim_content.npy\")\n",
    "\n",
    "# Save TF-IDF vectorizer\n",
    "if 'tfidf_vectorizer' in locals():\n",
    "    with open(path_tfidf_vectorizer, 'wb') as f:\n",
    "        pickle.dump(tfidf_vectorizer, f)\n",
    "    print(f\"Saved TF-IDF vectorizer to {path_tfidf_vectorizer}\")\n",
    "else:\n",
    "    print(\"TF-IDF vectorizer (tfidf_vectorizer) not found in memory, skipping save.\")\n",
    "\n",
    "# Save content similarity matrix (NumPy array)\n",
    "if 'cosine_sim_content' in locals():\n",
    "    np.save(path_cosine_sim_content, cosine_sim_content)\n",
    "    print(f\"Saved content cosine similarity matrix to {path_cosine_sim_content}\")\n",
    "else:\n",
    "    print(\"Content cosine similarity matrix (cosine_sim_content) not found in memory, skipping save.\")\n",
    "\n",
    "# Save content_movie_indices_lookup\n",
    "if 'content_movie_indices_lookup' in locals():\n",
    "    path_content_lookup = os.path.join(models_dir, \"content_movie_indices_lookup.pkl\")\n",
    "    with open(path_content_lookup, 'wb') as f:\n",
    "        pickle.dump(content_movie_indices_lookup, f)\n",
    "    print(f\"Saved content movie indices lookup to {path_content_lookup}\")\n",
    "else:\n",
    "    print(\"Content movie indices lookup (content_movie_indices_lookup) not found in memory, skipping save.\")\n",
    "\n",
    "print(\"--- Content-Based Components processed for saving. ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce92082f",
   "metadata": {},
   "source": [
    "## 6. Model Building - Part 3: Knowledge-Based/Graph-Based (Using Genome Tags)\n",
    "\n",
    "This section explores how to leverage the structured knowledge present in the MovieLens Genome dataset. The genome tags provide a rich, curated set of descriptors for movies, and their relevance scores indicate the strength of association. We can use this information in several ways:\n",
    "\n",
    "1.  **Movie Embeddings from Genome Tags:** Create vector representations (embeddings) for movies directly from their genome tag relevance scores. Movies with similar tag relevance patterns will be closer in the embedding space. This is a form of knowledge-based content filtering.\n",
    "2.  **Graph-Based Approaches:** Construct a graph where movies and tags are nodes, and edges represent relationships (e.g., a movie has a tag with a certain relevance). Graph embedding techniques (like Node2Vec, TransE) or graph traversal algorithms could then be used to find related movies or predict user preferences.\n",
    "3.  **Feature Augmentation:** The genome tag vectors or derived embeddings can be used as additional features in more complex hybrid models (e.g., factorization machines, neural networks).\n",
    "\n",
    "We'll start by creating movie embeddings from genome tag relevance scores and calculating similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "44840622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\n--- Building Knowledge-Based Model (Genome Tags) ---\n",
      "Movie-Genome Tag Matrix shape: (13816, 1128)\n",
      "Movie-Genome Tag Matrix shape: (13816, 1128)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\n--- Building Knowledge-Based Model (Genome Tags) ---\n",
      "Movie-Genome Tag Matrix shape: (13816, 1128)\n",
      "Movie-Genome Tag Matrix shape: (13816, 1128)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2365/2694930763.py:17: FutureWarning: The behavior of array concatenation with empty entries is deprecated. In a future version, this will no longer exclude empty items when determining the result dtype. To retain the old behavior, exclude the empty entries before the concat operation.\n",
      "  movie_genome_tag_matrix = movie_ids_from_movies_df.join(movie_genome_tag_matrix, how='left').fillna(0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\n--- Building Knowledge-Based Model (Genome Tags) ---\n",
      "Movie-Genome Tag Matrix shape: (13816, 1128)\n",
      "Movie-Genome Tag Matrix shape: (13816, 1128)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2365/2694930763.py:17: FutureWarning: The behavior of array concatenation with empty entries is deprecated. In a future version, this will no longer exclude empty items when determining the result dtype. To retain the old behavior, exclude the empty entries before the concat operation.\n",
      "  movie_genome_tag_matrix = movie_ids_from_movies_df.join(movie_genome_tag_matrix, how='left').fillna(0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aligned Movie-Genome Tag Matrix shape: (62423, 1128)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\n--- Building Knowledge-Based Model (Genome Tags) ---\n",
      "Movie-Genome Tag Matrix shape: (13816, 1128)\n",
      "Movie-Genome Tag Matrix shape: (13816, 1128)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2365/2694930763.py:17: FutureWarning: The behavior of array concatenation with empty entries is deprecated. In a future version, this will no longer exclude empty items when determining the result dtype. To retain the old behavior, exclude the empty entries before the concat operation.\n",
      "  movie_genome_tag_matrix = movie_ids_from_movies_df.join(movie_genome_tag_matrix, how='left').fillna(0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aligned Movie-Genome Tag Matrix shape: (62423, 1128)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "print(\"\\\\n--- Building Knowledge-Based Model (Genome Tags) ---\")\n",
    "\n",
    "# 1. Create Movie-Genome Tag Matrix\n",
    "# Pivot genome_scores_df to have movies as rows, tags as columns, and relevance as values\n",
    "if 'genome_scores_df' in locals() and 'genome_tags_df' in locals():\n",
    "    movie_genome_tag_matrix = genome_scores_df.pivot_table(\n",
    "        index='movieId',\n",
    "        columns='tagId',\n",
    "        values='relevance'\n",
    "    ).fillna(0) # Fill movies/tags with no score with 0\n",
    "\n",
    "    print(f\"Movie-Genome Tag Matrix shape: {movie_genome_tag_matrix.shape}\")\n",
    "\n",
    "    # Ensure all movies from movies_df are in this matrix, even if they have no genome scores\n",
    "    # (they will have all-zero vectors)\n",
    "    movie_ids_from_movies_df = movies_df[['movieId']].set_index('movieId')\n",
    "    movie_genome_tag_matrix = movie_ids_from_movies_df.join(movie_genome_tag_matrix, how='left').fillna(0)\n",
    "    print(f\"Aligned Movie-Genome Tag Matrix shape: {movie_genome_tag_matrix.shape}\")\n",
    "\n",
    "    # 2. Compute Cosine Similarity from Genome Tag Vectors\n",
    "    # Note: For a large number of movies (e.g., 62k in ML-25M), this matrix (num_movies x num_movies)\n",
    "    # can be large (e.g., ~15GB for float32). Consider alternatives like approximate nearest neighbors\n",
    "    # if memory becomes an issue.\n",
    "    cosine_sim_genome = cosine_similarity(movie_genome_tag_matrix)\n",
    "    print(f\"Genome-based cosine similarity matrix shape: {cosine_sim_genome.shape}\")\n",
    "\n",
    "    # Create a mapping from movieId to index in the movie_genome_tag_matrix for easy lookup\n",
    "    genome_movie_ids = movie_genome_tag_matrix.index.tolist()\n",
    "    genome_movie_indices = {movieId: i for i, movieId in enumerate(genome_movie_ids)}\n",
    "\n",
    "    # 3. Function to get recommendations based on genome tag similarity\n",
    "    def get_genome_recommendations(movie_id, num_recs=10):\n",
    "        if movie_id not in genome_movie_indices:\n",
    "            print(f\"MovieId {movie_id} not found in genome matrix.\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        movie_idx = genome_movie_indices[movie_id]\n",
    "        sim_scores = list(enumerate(cosine_sim_genome[movie_idx]))\n",
    "        sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "        sim_scores = sim_scores[1:num_recs+1] # Exclude the movie itself\n",
    "\n",
    "        recommended_movie_indices_genome = [i[0] for i in sim_scores]\n",
    "        recommended_scores_ordered = [s[1] for s in sim_scores] # Scores in order\n",
    "\n",
    "        # Map these integer indices back to movieIds using genome_movie_ids list.\n",
    "        # This requires genome_movie_ids to be available and in sync with the matrix.\n",
    "        recommended_movie_ids = [genome_movie_ids[i] for i in recommended_movie_indices_genome if i < len(genome_movie_ids)]\n",
    "        \n",
    "        # Get movie details from movies_df, ensuring the order of recommended_movie_ids\n",
    "        # and handling cases where a recommended movieId might not be in the main movies_df (if it was filtered out previously)\n",
    "        # However, movie_genome_tag_matrix is aligned with movies_df's movieIds, so they should all be present.\n",
    "        recommended_movies = movies_df[movies_df['movieId'].isin(recommended_movie_ids)].set_index('movieId').loc[recommended_movie_ids].reset_index()\n",
    "        \n",
    "        # Assign scores directly as they are already ordered\n",
    "        recommended_movies['genome_similarity_score'] = recommended_scores_ordered\n",
    "        \n",
    "        return recommended_movies[['movieId', 'title_cleaned', 'genres_list', 'genome_similarity_score']]\n",
    "\n",
    "    # 4. Example Usage\n",
    "    if not movies_df.empty:\n",
    "        # Ensure the example movie ID exists in genome_movie_indices to prevent errors\n",
    "        example_movie_id_for_genome = None\n",
    "        if len(movies_df) > 10 and movies_df['movieId'].iloc[10] in genome_movie_indices:\n",
    "            example_movie_id_for_genome = movies_df['movieId'].iloc[10] \n",
    "        elif not genome_movie_indices.empty: # Fallback to the first movie in the genome index\n",
    "             example_movie_id_for_genome = genome_movie_indices.index[0]\n",
    "\n",
    "        if example_movie_id_for_genome:\n",
    "            print(f\"\\\\\\\\nExample Genome-Based Recommendations for movie: {movies_df[movies_df['movieId'] == example_movie_id_for_genome]['title_cleaned'].values[0]} (ID: {example_movie_id_for_genome})\")\n",
    "            genome_recs_example = get_genome_recommendations(example_movie_id_for_genome, 5)\n",
    "            if not genome_recs_example.empty:\n",
    "                print(genome_recs_example)\n",
    "            else:\n",
    "                print(f\"Could not generate genome recommendations for movieId {example_movie_id_for_genome}.\")\n",
    "        else:\n",
    "            print(\"Could not find a suitable example movie ID for genome recommendations.\")\n",
    "    else:\n",
    "        print(\"movies_df is empty, cannot run genome recommendation example.\")\n",
    "else:\n",
    "    print(\"Genome data (genome_scores_df or genome_tags_df) not loaded. Skipping genome-based model.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3172bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Saving Knowledge-Based Components ---\")\n",
    "models_dir = \"../models/\"\n",
    "if not os.path.exists(models_dir):\n",
    "    os.makedirs(models_dir)\n",
    "    print(f\"Created directory: {models_dir}\")\n",
    "\n",
    "path_movie_genome_matrix = os.path.join(models_dir, \"adv_movie_genome_tag_matrix.parquet\")\n",
    "path_cosine_sim_genome = os.path.join(models_dir, \"adv_cosine_sim_genome.npy\")\n",
    "\n",
    "# Save movie-genome tag matrix (Pandas DataFrame to Parquet)\n",
    "if 'movie_genome_tag_matrix' in locals() and isinstance(movie_genome_tag_matrix, pd.DataFrame):\n",
    "    try:\n",
    "        movie_genome_tag_matrix.to_parquet(path_movie_genome_matrix)\n",
    "        print(f\"Saved movie-genome tag matrix to {path_movie_genome_matrix}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving movie-genome tag matrix: {e}\")\n",
    "elif 'movie_genome_tag_matrix' not in locals():\n",
    "    print(\"Movie-genome tag matrix (movie_genome_tag_matrix) not found in memory, skipping save.\")\n",
    "else:\n",
    "    print(f\"movie_genome_tag_matrix is not a DataFrame (type: {type(movie_genome_tag_matrix)}), skipping save.\")\n",
    "\n",
    "# Save genome similarity matrix (NumPy array)\n",
    "if 'cosine_sim_genome' in locals():\n",
    "    np.save(path_cosine_sim_genome, cosine_sim_genome)\n",
    "    print(f\"Saved genome cosine similarity matrix to {path_cosine_sim_genome}\")\n",
    "else:\n",
    "    print(\"Genome cosine similarity matrix (cosine_sim_genome) not found in memory, skipping save.\")\n",
    "print(\"--- Knowledge-Based Components processed for saving. ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d591f65",
   "metadata": {},
   "source": [
    "## 7. Hybridization Strategy\n",
    "\n",
    "After developing individual recommendation models (Collaborative Filtering, Content-Based, Knowledge-Based), the next step is to combine their strengths through hybridization. A hybrid recommender can often outperform individual models by leveraging different sources of information and mitigating the weaknesses of each approach.\n",
    "\n",
    "We will explore **Weighted Blending**, a common hybridization technique.\n",
    "\n",
    "### Weighted Blending\n",
    "\n",
    "In this approach, we take the rating predictions from multiple models and combine them using a weighted average. For a user *u* and an item *i*, the hybrid prediction can be calculated as:\n",
    "\n",
    "`HybridScore(u, i) = w_1 * Score_1(u, i) + w_2 * Score_2(u, i) + ... + w_n * Score_n(u, i)`\n",
    "\n",
    "Where:\n",
    "*   `Score_k(u, i)` is the prediction (e.g., rating) for user *u* on item *i* from model *k*.\n",
    "*   `w_k` is the weight assigned to model *k*.\n",
    "*   The sum of all weights (`w_1 + w_2 + ... + w_n`) typically equals 1.\n",
    "\n",
    "The weights can be determined empirically, through domain knowledge, or optimized by evaluating the hybrid model's performance on a validation dataset.\n",
    "\n",
    "Our goal will be to combine:\n",
    "1.  **Collaborative Filtering (SVD) predictions.**\n",
    "2.  **Content-Based predictions** (derived from item similarity based on TF-IDF of genres and genome tag profiles).\n",
    "3.  **Knowledge-Based predictions** (derived from item similarity based on genome tag relevance vectors).\n",
    "\n",
    "To use the content-based and knowledge-based models (which provide item-item similarity) for rating prediction, we can estimate a user's rating for a target item by looking at the ratings they've given to similar items. A common formula is:\n",
    "\n",
    "`Pred(u, i) = sum_j ( sim(i, j) * rating(u, j) ) / sum_j ( sim(i, j) )`\n",
    "where *j* represents items that user *u* has rated and are similar to item *i*, and `sim(i, j)` is the similarity between item *i* and item *j*. We usually consider only the top *k* most similar items *j* for this calculation.\n",
    "\n",
    "Let's implement this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1822326e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data and models for hybridization...\n",
      "movies_df already in memory.\n",
      "ratings_df already in memory.\n",
      "Loading SVD model from /workspaces/soe-python/models/adv_algo_svd.pkl...\n",
      "SVD model loaded.\n",
      "trainset_surprise already in memory and seems valid.\n",
      "SVD model loaded.\n",
      "trainset_surprise already in memory and seems valid.\n",
      "Loading content cosine similarity from /workspaces/soe-python/models/adv_cosine_sim_content.npy using mmap_mode='r'...\n",
      "Content cosine similarity loaded. Shape: (62423, 62423).\n",
      "Loading content cosine similarity from /workspaces/soe-python/models/adv_cosine_sim_content.npy using mmap_mode='r'...\n",
      "Content cosine similarity loaded. Shape: (62423, 62423).\n",
      "Loading content movie indices lookup from /workspaces/soe-python/models/content_movie_indices_lookup.pkl...\n",
      "Content movie indices lookup loaded.\n",
      "Loading content movie indices lookup from /workspaces/soe-python/models/content_movie_indices_lookup.pkl...\n",
      "Content movie indices lookup loaded.\n",
      "Content-based components (cosine similarity, lookup) loaded.\n",
      "Loading genome cosine similarity from /workspaces/soe-python/models/adv_cosine_sim_genome.npy using mmap_mode='r'...\n",
      "Genome cosine similarity loaded. Shape: (62423, 62423).\n",
      "Content-based components (cosine similarity, lookup) loaded.\n",
      "Loading genome cosine similarity from /workspaces/soe-python/models/adv_cosine_sim_genome.npy using mmap_mode='r'...\n",
      "Genome cosine similarity loaded. Shape: (62423, 62423).\n",
      "Loading movie_genome_tag_matrix from /workspaces/soe-python/models/adv_movie_genome_tag_matrix.parquet to get genome_movie_ids...\n",
      "Loading movie_genome_tag_matrix from /workspaces/soe-python/models/adv_movie_genome_tag_matrix.parquet to get genome_movie_ids...\n",
      "Genome movie indices created from loaded matrix.\n",
      "Knowledge-based components (cosine similarity, lookup) loaded.\n",
      "\n",
      "--- All data and models loaded successfully (or attempted). ---\n",
      "Hybrid prediction function defined.\n",
      "\n",
      "Getting hybrid prediction for User 1, Movie 318...\n",
      "Genome movie indices created from loaded matrix.\n",
      "Knowledge-based components (cosine similarity, lookup) loaded.\n",
      "\n",
      "--- All data and models loaded successfully (or attempted). ---\n",
      "Hybrid prediction function defined.\n",
      "\n",
      "Getting hybrid prediction for User 1, Movie 318...\n",
      "Predicting for User: 134814, Movie: 180\n",
      "  Hybrid Score for 'Mallrats (1995)' (ID 180): 3.6694\n",
      "  Individual Scores: {'svd': 3.8960924834820574, 'content': 3.4733907019592807, 'kb': 3.56310589871031}\n",
      "\n",
      "--- Hybridization Cell Execution Complete ---\n",
      "Predicting for User: 134814, Movie: 180\n",
      "  Hybrid Score for 'Mallrats (1995)' (ID 180): 3.6694\n",
      "  Individual Scores: {'svd': 3.8960924834820574, 'content': 3.4733907019592807, 'kb': 3.56310589871031}\n",
      "\n",
      "--- Hybridization Cell Execution Complete ---\n"
     ]
    }
   ],
   "source": [
    "# Section 7: Hybridization Strategy - Weighted Blending\n",
    "# ----------------------------------------------------\n",
    "# This section implements a hybrid recommender system by blending the predictions\n",
    "# from the SVD (Collaborative Filtering), Content-Based, and Knowledge-Based models.\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from surprise import Dataset, Reader\n",
    "from surprise.model_selection import train_test_split as surprise_train_test_split\n",
    "import gc # Import garbage collector\n",
    "\n",
    "# Define file paths for data and models\n",
    "data_path = \"/workspaces/soe-python/data\"\n",
    "models_path = \"/workspaces/soe-python/models\"\n",
    "parquet_path = os.path.join(data_path, \"parquet\")\n",
    "\n",
    "# --- 1. Load necessary data and models sequentially ---\n",
    "print(\"Loading data and models for hybridization...\")\n",
    "gc.collect()\n",
    "\n",
    "# Load movies_df\n",
    "if 'movies_df' not in globals() or movies_df.empty:\n",
    "    print(\"Loading movies_df from Parquet...\")\n",
    "    movies_df = pd.read_parquet(os.path.join(parquet_path, 'movies.parquet'))\n",
    "    # Basic preprocessing if loaded fresh\n",
    "    movies_df['year'] = movies_df['title'].str.extract(r'\\((\\d{4})\\)\\s*$', expand=False)\n",
    "    movies_df['year'] = pd.to_numeric(movies_df['year'], errors='coerce')\n",
    "    def clean_title_simple(title):\n",
    "        if not isinstance(title, str):\n",
    "            title = str(title)\n",
    "        title_no_year = re.sub(r'\\s*\\(\\d{4}\\)\\s*$', '', title).strip()\n",
    "        return re.sub(r\"[^a-zA-Z0-9\\s:,&.'-]\", '', title_no_year).strip()\n",
    "    movies_df['title_cleaned'] = movies_df['title'].apply(clean_title_simple)\n",
    "    movies_df['genres_list'] = movies_df['genres'].apply(lambda x: x.split('|') if isinstance(x, str) else [])\n",
    "    # Add movie_avg_rating if not present (e.g. from previous cells)\n",
    "    if 'movie_avg_rating' not in movies_df.columns and 'ratings_df' in globals():\n",
    "        movie_avg_ratings_series = ratings_df.groupby('movieId')['rating'].mean().rename('movie_avg_rating')\n",
    "        movies_df = movies_df.merge(movie_avg_ratings_series, on='movieId', how='left')\n",
    "        movies_df['movie_avg_rating'] = movies_df['movie_avg_rating'].fillna(0)\n",
    "\n",
    "    print(f\"movies_df loaded/processed. Shape: {movies_df.shape}. Memory usage: {movies_df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "    gc.collect()\n",
    "else:\n",
    "    print(\"movies_df already in memory.\")\n",
    "\n",
    "# Load ratings_df\n",
    "if 'ratings_df' not in globals() or ratings_df.empty:\n",
    "    print(\"Loading ratings_df from Parquet...\")\n",
    "    ratings_df = pd.read_parquet(os.path.join(parquet_path, 'ratings.parquet'))\n",
    "    ratings_df['rating'] = pd.to_numeric(ratings_df['rating'], errors='coerce')\n",
    "    ratings_df.dropna(subset=['rating'], inplace=True)\n",
    "    print(f\"ratings_df loaded. Shape: {ratings_df.shape}. Memory usage: {ratings_df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "    gc.collect()\n",
    "else:\n",
    "    print(\"ratings_df already in memory.\")\n",
    "\n",
    "\n",
    "# Collaborative Filtering (SVD)\n",
    "svd_model_path = os.path.join(models_path, 'adv_algo_svd.pkl')\n",
    "print(f\"Loading SVD model from {svd_model_path}...\")\n",
    "with open(svd_model_path, 'rb') as f:\n",
    "    algo_svd = pickle.load(f)\n",
    "print(\"SVD model loaded.\")\n",
    "gc.collect()\n",
    "\n",
    "# Recreate trainset_surprise if not in memory or if its components are missing\n",
    "# Check for trainset_surprise and its attributes like rating_scale\n",
    "recreate_trainset = False\n",
    "if 'trainset_surprise' not in globals():\n",
    "    recreate_trainset = True\n",
    "else:\n",
    "    try:\n",
    "        _ = trainset_surprise.rating_scale # Check if it has necessary attributes\n",
    "        _ = trainset_surprise.global_mean\n",
    "        print(\"trainset_surprise already in memory and seems valid.\")\n",
    "    except AttributeError:\n",
    "        print(\"trainset_surprise found in memory but seems incomplete or corrupted. Recreating...\")\n",
    "        recreate_trainset = True\n",
    "\n",
    "if recreate_trainset:\n",
    "    print(\"Recreating trainset_surprise for SVD...\")\n",
    "    if 'ratings_df' in globals() and not ratings_df.empty:\n",
    "        # Ensure ratings_df has the required columns and data types for Surprise\n",
    "        ratings_df_surprise = ratings_df[['userId', 'movieId', 'rating']].copy()\n",
    "        ratings_df_surprise['rating'] = pd.to_numeric(ratings_df_surprise['rating'], errors='coerce')\n",
    "        ratings_df_surprise.dropna(subset=['rating'], inplace=True)\n",
    "        \n",
    "        min_rating_val = ratings_df_surprise['rating'].min()\n",
    "        max_rating_val = ratings_df_surprise['rating'].max()\n",
    "        \n",
    "        if pd.isna(min_rating_val) or pd.isna(max_rating_val):\n",
    "            print(\"Error: Could not determine rating scale from ratings_df. Using default 0.5-5.0.\")\n",
    "            min_rating_val, max_rating_val = 0.5, 5.0 # Fallback\n",
    "            \n",
    "        reader = Reader(rating_scale=(min_rating_val, max_rating_val))\n",
    "        data_surprise = Dataset.load_from_df(ratings_df_surprise, reader)\n",
    "        trainset_surprise = data_surprise.build_full_trainset()\n",
    "        print(f\"trainset_surprise recreated. Users: {trainset_surprise.n_users}, Items: {trainset_surprise.n_items}, Ratings: {trainset_surprise.n_ratings}\")\n",
    "        print(f\"Rating scale: {trainset_surprise.rating_scale}, Global mean: {trainset_surprise.global_mean}\")\n",
    "    else:\n",
    "        print(\"ratings_df not available to recreate trainset_surprise. This might cause critical issues.\")\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "# Content-Based Filtering\n",
    "# tfidf_vectorizer_path = os.path.join(models_path, 'adv_tfidf_vectorizer.pkl') # Not directly passed\n",
    "cosine_sim_content_path = os.path.join(models_path, 'adv_cosine_sim_content.npy')\n",
    "content_lookup_path = os.path.join(models_path, 'content_movie_indices_lookup.pkl')\n",
    "\n",
    "# print(f\"Loading TF-IDF vectorizer from {tfidf_vectorizer_path}...\") # Not strictly needed if not passed\n",
    "# with open(tfidf_vectorizer_path, 'rb') as f:\n",
    "#     tfidf_vectorizer = pickle.load(f)\n",
    "# print(\"TF-IDF vectorizer loaded.\")\n",
    "gc.collect()\n",
    "\n",
    "print(f\"Loading content cosine similarity from {cosine_sim_content_path} using mmap_mode='r'...\")\n",
    "cosine_sim_content = np.load(cosine_sim_content_path, mmap_mode='r')\n",
    "print(f\"Content cosine similarity loaded. Shape: {cosine_sim_content.shape}.\")\n",
    "gc.collect()\n",
    "\n",
    "print(f\"Loading content movie indices lookup from {content_lookup_path}...\")\n",
    "with open(content_lookup_path, 'rb') as f:\n",
    "    content_movie_indices_lookup = pickle.load(f)\n",
    "print(\"Content movie indices lookup loaded.\")\n",
    "gc.collect()\n",
    "\n",
    "print(\"Content-based components (cosine similarity, lookup) loaded.\")\n",
    "\n",
    "\n",
    "# Knowledge-Based Filtering (using MovieLens Tag Genome)\n",
    "# movie_genome_matrix_path = os.path.join(models_path, 'adv_movie_genome_tag_matrix.parquet') # Not directly passed\n",
    "cosine_sim_genome_path = os.path.join(models_path, 'adv_cosine_sim_genome.npy')\n",
    "\n",
    "# print(f\"Loading movie-genome tag matrix from {movie_genome_matrix_path}...\") # Not strictly needed if not passed\n",
    "# movie_genome_tag_matrix = pd.read_parquet(movie_genome_matrix_path)\n",
    "# print(f\"Movie-genome tag matrix loaded. Shape: {movie_genome_tag_matrix.shape}.\")\n",
    "gc.collect()\n",
    "\n",
    "print(f\"Loading genome cosine similarity from {cosine_sim_genome_path} using mmap_mode='r'...\")\n",
    "cosine_sim_genome = np.load(cosine_sim_genome_path, mmap_mode='r')\n",
    "print(f\"Genome cosine similarity loaded. Shape: {cosine_sim_genome.shape}.\")\n",
    "gc.collect()\n",
    "\n",
    "# Create a lookup for movieId to index in the genome similarity matrix\n",
    "# This requires movie_genome_tag_matrix to be loaded if genome_movie_ids is not saved separately\n",
    "# For now, assuming genome_movie_indices is self-sufficient or created if needed.\n",
    "# If movie_genome_tag_matrix was loaded, genome_movie_ids would be: movie_genome_tag_matrix.index.tolist()\n",
    "# We need a robust way to get genome_movie_ids if it's not passed or saved.\n",
    "# Let's try to load it or infer it if `adv_movie_genome_tag_matrix.parquet` exists\n",
    "path_adv_movie_genome_tag_matrix = os.path.join(models_path, \"adv_movie_genome_tag_matrix.parquet\")\n",
    "if os.path.exists(path_adv_movie_genome_tag_matrix):\n",
    "    print(f\"Loading movie_genome_tag_matrix from {path_adv_movie_genome_tag_matrix} to get genome_movie_ids...\")\n",
    "    temp_genome_matrix = pd.read_parquet(path_adv_movie_genome_tag_matrix)\n",
    "    genome_movie_ids = temp_genome_matrix.index.tolist()\n",
    "    del temp_genome_matrix # Free memory\n",
    "    gc.collect()\n",
    "    genome_movie_indices = {movieId: i for i, movieId in enumerate(genome_movie_ids)}\n",
    "    print(\"Genome movie indices created from loaded matrix.\")\n",
    "elif 'genome_movie_indices' not in globals():\n",
    "    print(\"Warning: genome_movie_indices not found and cannot be created as adv_movie_genome_tag_matrix.parquet is missing.\")\n",
    "    genome_movie_indices = {} # Fallback to empty dict\n",
    "else:\n",
    "    print(\"genome_movie_indices already in memory or assumed to be loaded correctly.\")\n",
    "\n",
    "print(\"Knowledge-based components (cosine similarity, lookup) loaded.\")\n",
    "gc.collect()\n",
    "\n",
    "print(\"\\n--- All data and models loaded successfully (or attempted). ---\")\n",
    "\n",
    "\n",
    "# --- 2. Define the Hybrid Prediction Function ---\n",
    "def hybrid_prediction(userId, movieId, movies_df, ratings_df, algo_svd, trainset_surprise,\n",
    "                      cosine_sim_content, content_movie_indices_lookup,\n",
    "                      cosine_sim_genome, genome_movie_indices, # Added genome_movie_ids\n",
    "                      weights=(0.4, 0.3, 0.3), k_similar=10):\n",
    "    \"\"\"\n",
    "    Generates a hybrid recommendation score for a given user and movie.\n",
    "    Uses user's ratings on similar items for content and knowledge-based scores.\n",
    "\n",
    "    Args:\n",
    "        userId (int): The ID of the user.\n",
    "        movieId (int): The ID of the movie.\n",
    "        movies_df (pd.DataFrame): DataFrame containing movie information.\n",
    "        ratings_df (pd.DataFrame): DataFrame containing user ratings.\n",
    "        algo_svd: Trained SVD model from Surprise.\n",
    "        trainset_surprise: Surprise trainset.\n",
    "        cosine_sim_content (np.array): Cosine similarity matrix for content-based filtering.\n",
    "        content_movie_indices_lookup (pd.Series): Lookup Series for movie titles to content matrix indices.\n",
    "        cosine_sim_genome (np.array): Cosine similarity matrix for genome-based filtering.\n",
    "        genome_movie_indices (dict): Lookup for movieId to genome matrix indices.\n",
    "        weights (tuple): Weights for SVD, content-based, and knowledge-based predictions.\n",
    "        k_similar (int): Number of similar items to consider for content/KB predictions.\n",
    "\n",
    "    Returns:\n",
    "        float: The hybrid recommendation score.\n",
    "        dict: Individual scores from each model.\n",
    "    \"\"\"\n",
    "    w_svd, w_content, w_kb = weights\n",
    "    individual_scores = {}\n",
    "    min_rating, max_rating = trainset_surprise.rating_scale\n",
    "\n",
    "    # SVD Prediction\n",
    "    svd_pred = trainset_surprise.global_mean # Default to global mean\n",
    "    try:\n",
    "        # Check if user and item are known to the trainset\n",
    "        if trainset_surprise.knows_user(userId) and trainset_surprise.knows_item(movieId):\n",
    "            inner_user_id = trainset_surprise.to_inner_uid(userId)\n",
    "            inner_movie_id = trainset_surprise.to_inner_iid(movieId)\n",
    "            svd_pred = algo_svd.predict(inner_user_id, inner_movie_id).est\n",
    "        elif movieId in movies_df['movieId'].values:\n",
    "             # Fallback: movie's average rating if known, else global mean\n",
    "            movie_avg_rating_val = movies_df.loc[movies_df['movieId'] == movieId, 'movie_avg_rating'].iloc[0]\n",
    "            if pd.notna(movie_avg_rating_val) and movie_avg_rating_val > 0:\n",
    "                svd_pred = movie_avg_rating_val\n",
    "            # else svd_pred remains global_mean\n",
    "    except ValueError: # Handles cases where user/item might not be convertible to inner IDs\n",
    "        # print(f\"SVD ValueError for user {userId}, movie {movieId}. Using fallback.\")\n",
    "        if movieId in movies_df['movieId'].values:\n",
    "            movie_avg_rating_val = movies_df.loc[movies_df['movieId'] == movieId, 'movie_avg_rating'].iloc[0]\n",
    "            if pd.notna(movie_avg_rating_val) and movie_avg_rating_val > 0:\n",
    "                svd_pred = movie_avg_rating_val\n",
    "    except Exception as e:\n",
    "        # print(f\"Unexpected SVD error for user {userId}, movie {movieId}: {e}. Using fallback.\")\n",
    "        if movieId in movies_df['movieId'].values:\n",
    "            movie_avg_rating_val = movies_df.loc[movies_df['movieId'] == movieId, 'movie_avg_rating'].iloc[0]\n",
    "            if pd.notna(movie_avg_rating_val) and movie_avg_rating_val > 0:\n",
    "                svd_pred = movie_avg_rating_val\n",
    "    individual_scores['svd'] = svd_pred\n",
    "\n",
    "    # Get ratings by the current user\n",
    "    user_ratings = ratings_df[ratings_df['userId'] == userId]\n",
    "\n",
    "    # Content-Based Prediction\n",
    "    content_pred = svd_pred # Default to SVD if content score cannot be calculated\n",
    "    try:\n",
    "        # Ensure movie_title can be found\n",
    "        movie_details = movies_df[movies_df['movieId'] == movieId]\n",
    "        if not movie_details.empty:\n",
    "            movie_title = movie_details['title'].iloc[0] # Use original title as per lookup creation\n",
    "            if movie_title in content_movie_indices_lookup:\n",
    "                idx = content_movie_indices_lookup[movie_title]\n",
    "                sim_scores_content = list(enumerate(cosine_sim_content[idx]))\n",
    "                sim_scores_content = sorted(sim_scores_content, key=lambda x: x[1], reverse=True)[1:k_similar+1]\n",
    "                \n",
    "                similar_movie_indices_content = [i[0] for i in sim_scores_content]\n",
    "                # These indices are for movies_df rows at the time of TF-IDF fitting.\n",
    "                # Assuming movies_df order hasn't changed or lookup is robust.\n",
    "                similar_movie_ids_content = movies_df.iloc[similar_movie_indices_content]['movieId'].values\n",
    "\n",
    "                rated_similar_movies_content = user_ratings[user_ratings['movieId'].isin(similar_movie_ids_content)]\n",
    "                \n",
    "                if not rated_similar_movies_content.empty:\n",
    "                    weighted_sum_ratings_c = 0\n",
    "                    sum_similarities_c = 0\n",
    "                    for _, row_c in rated_similar_movies_content.iterrows():\n",
    "                        rated_movie_id_c = row_c['movieId']\n",
    "                        rating_c = row_c['rating']\n",
    "                        try:\n",
    "                            rated_movie_title_c = movies_df.loc[movies_df['movieId'] == rated_movie_id_c, 'title'].iloc[0]\n",
    "                            if rated_movie_title_c in content_movie_indices_lookup:\n",
    "                                rated_movie_idx_in_content_c = content_movie_indices_lookup[rated_movie_title_c]\n",
    "                                similarity_c = cosine_sim_content[idx, rated_movie_idx_in_content_c]\n",
    "                                weighted_sum_ratings_c += similarity_c * rating_c\n",
    "                                sum_similarities_c += similarity_c\n",
    "                        except (IndexError, KeyError):\n",
    "                            continue \n",
    "                    \n",
    "                    if sum_similarities_c > 0:\n",
    "                        content_pred = weighted_sum_ratings_c / sum_similarities_c\n",
    "            # else: movie_title not in lookup, content_pred remains svd_pred\n",
    "        # else: movie not in movies_df, content_pred remains svd_pred\n",
    "    except (IndexError, KeyError, TypeError) as e:\n",
    "        # print(f\"Content-based prediction error for movie {movieId} (user {userId}): {e}\")\n",
    "        pass # content_pred remains svd_pred\n",
    "    individual_scores['content'] = np.clip(content_pred, min_rating, max_rating)\n",
    "\n",
    "    # Knowledge-Based (Genome) Prediction\n",
    "    kb_pred = svd_pred # Default to SVD\n",
    "    try:\n",
    "        if movieId in genome_movie_indices and genome_movie_ids: # Ensure genome_movie_ids is available\n",
    "            idx_genome = genome_movie_indices[movieId]\n",
    "            sim_scores_genome = list(enumerate(cosine_sim_genome[idx_genome]))\n",
    "            sim_scores_genome = sorted(sim_scores_genome, key=lambda x: x[1], reverse=True)[1:k_similar+1]\n",
    "\n",
    "            similar_genome_matrix_indices = [i[0] for i in sim_scores_genome]\n",
    "            # These indices are for the cosine_sim_genome matrix.\n",
    "            # We need to map them to movieIds using genome_movie_ids list.\n",
    "            similar_movie_ids_genome = [genome_movie_ids[i] for i in similar_genome_matrix_indices if i < len(genome_movie_ids)]\n",
    "            \n",
    "            rated_similar_movies_genome = user_ratings[user_ratings['movieId'].isin(similar_movie_ids_genome)]\n",
    "\n",
    "            if not rated_similar_movies_genome.empty:\n",
    "                weighted_sum_ratings_kb = 0\n",
    "                sum_similarities_kb = 0\n",
    "                for _, row_kb in rated_similar_movies_genome.iterrows():\n",
    "                    rated_movie_id_kb = row_kb['movieId']\n",
    "                    rating_kb = row_kb['rating']\n",
    "                    try:\n",
    "                        if rated_movie_id_kb in genome_movie_indices:\n",
    "                            rated_movie_idx_in_genome_kb = genome_movie_indices[rated_movie_id_kb]\n",
    "                            similarity_kb = cosine_sim_genome[idx_genome, rated_movie_idx_in_genome_kb]\n",
    "                            weighted_sum_ratings_kb += similarity_kb * rating_kb\n",
    "                            sum_similarities_kb += similarity_kb\n",
    "                    except (IndexError, KeyError):\n",
    "                        continue\n",
    "                \n",
    "                if sum_similarities_kb > 0:\n",
    "                    kb_pred = weighted_sum_ratings_kb / sum_similarities_kb\n",
    "            # else: user hasn't rated similar genome items, kb_pred remains svd_pred\n",
    "        # else: movie not in genome_movie_indices, kb_pred remains svd_pred\n",
    "    except (IndexError, KeyError, TypeError) as e:\n",
    "        # print(f\"Knowledge-based prediction error for movie {movieId} (user {userId}): {e}\")\n",
    "        pass # kb_pred remains svd_pred\n",
    "    individual_scores['kb'] = np.clip(kb_pred, min_rating, max_rating)\n",
    "\n",
    "    # Weighted Hybrid Score\n",
    "    hybrid_score = (w_svd * individual_scores['svd']) + \\\n",
    "                   (w_content * individual_scores['content']) + \\\n",
    "                   (w_kb * individual_scores['kb'])\n",
    "\n",
    "    hybrid_score = np.clip(hybrid_score, min_rating, max_rating)\n",
    "\n",
    "    return hybrid_score, individual_scores\n",
    "\n",
    "print(\"Hybrid prediction function defined.\")\n",
    "\n",
    "# --- 3. Example: Get hybrid predictions for a sample user and item ---\n",
    "\n",
    "sample_user_id = 1\n",
    "sample_movie_id = 318 \n",
    "\n",
    "if 'movies_df' not in globals() or movies_df.empty:\n",
    "    print(\"movies_df is empty, cannot pick a sample movie for example.\")\n",
    "elif sample_movie_id not in movies_df['movieId'].values:\n",
    "    print(f\"Movie with ID {sample_movie_id} not found in movies_df. Using first movie as default.\")\n",
    "    sample_movie_id = movies_df['movieId'].iloc[0]\n",
    "\n",
    "if 'ratings_df' not in globals() or ratings_df.empty:\n",
    "    print(\"ratings_df not available to check sample_user_id for example.\")\n",
    "elif sample_user_id not in ratings_df['userId'].unique():\n",
    "    print(f\"User with ID {sample_user_id} not found in ratings_df. Using first user as default.\")\n",
    "    sample_user_id = ratings_df['userId'].unique()[0]\n",
    "\n",
    "print(f\"\\nGetting hybrid prediction for User {sample_user_id}, Movie {sample_movie_id}...\")\n",
    "\n",
    "items_to_predict = []\n",
    "if 'ratings_df' in globals() and not ratings_df.empty and \\\n",
    "   'movies_df' in globals() and not movies_df.empty and \\\n",
    "   'trainset_surprise' in globals() and \\\n",
    "   'genome_movie_ids' in globals() and genome_movie_ids: # Ensure genome_movie_ids is ready\n",
    "    try:\n",
    "        sample_interaction = ratings_df.sample(1).iloc[0]\n",
    "        items_to_predict = [(int(sample_interaction['userId']), int(sample_interaction['movieId']))]\n",
    "    except Exception as e:\n",
    "        print(f\"Could not get sample interaction: {e}\")\n",
    "        items_to_predict = [(sample_user_id, sample_movie_id)] # Fallback to predefined sample\n",
    "else:\n",
    "    print(\"Skipping example prediction due to missing dataframes or trainset_surprise or genome_movie_ids.\")\n",
    "    items_to_predict = [(sample_user_id, sample_movie_id)] # Attempt with predefined if data is missing\n",
    "\n",
    "if items_to_predict:\n",
    "    user_id_ex, movie_id_ex = items_to_predict[0]\n",
    "    print(f\"Predicting for User: {user_id_ex}, Movie: {movie_id_ex}\")\n",
    "    try:\n",
    "        # Check all required components for the function call\n",
    "        if not all(k in globals() for k in ['movies_df', 'ratings_df', 'algo_svd', 'trainset_surprise',\n",
    "                                            'cosine_sim_content', 'content_movie_indices_lookup',\n",
    "                                            'cosine_sim_genome', 'genome_movie_indices', 'genome_movie_ids']):\n",
    "            print(\"Error: One or more model components are missing for the example prediction.\")\n",
    "        elif not genome_movie_ids: # Specific check for genome_movie_ids list\n",
    "             print(\"Error: genome_movie_ids list is empty. Cannot make example prediction.\")\n",
    "        else:\n",
    "            hybrid_score, individual_scores = hybrid_prediction(\n",
    "                userId=user_id_ex,\n",
    "                movieId=movie_id_ex,\n",
    "                movies_df=movies_df,\n",
    "                ratings_df=ratings_df,\n",
    "                algo_svd=algo_svd,\n",
    "                trainset_surprise=trainset_surprise,\n",
    "                cosine_sim_content=cosine_sim_content,\n",
    "                content_movie_indices_lookup=content_movie_indices_lookup,\n",
    "                cosine_sim_genome=cosine_sim_genome,\n",
    "                genome_movie_indices=genome_movie_indices,\n",
    "                genome_movie_ids=genome_movie_ids, # Pass the list of movie IDs for genome\n",
    "                weights=(0.4, 0.3, 0.3)\n",
    "            )\n",
    "            movie_title_display = \"Unknown Movie\"\n",
    "            if 'movies_df' in globals() and movie_id_ex in movies_df['movieId'].values:\n",
    "                 movie_title_display = movies_df.loc[movies_df['movieId'] == movie_id_ex, 'title'].iloc[0]\n",
    "            \n",
    "            print(f\"  Hybrid Score for '{movie_title_display}' (ID {movie_id_ex}): {hybrid_score:.4f}\")\n",
    "            print(f\"  Individual Scores: {individual_scores}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during example prediction for User {user_id_ex}, Movie {movie_id_ex}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "else:\n",
    "    print(\"No items to predict for the example. Skipping example prediction block.\")\n",
    "\n",
    "print(\"\\n--- Hybridization Cell Execution Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c104fc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Implementing Ranking Metrics ---\n",
      "\\n--- Setting up for Ranking Metrics Evaluation ---\n",
      "All necessary components for ranking evaluation appear to be present.\n",
      "Identified 158667 users with relevant items in the test set (rating >= 4.0).\n",
      "Will evaluate ranking metrics for 100 users.\n",
      "Using hybrid_weights for ranking: (0.7, 0.15, 0.15)\n",
      "\\n--- DIAGNOSTICS FOR FIRST USER: 81933 ---\n",
      "Processing user 1/100 (ID: 81933) for diagnostics.\n",
      "True relevant items: {34048, 44929, 1291, 780, 527, 33679, 39446, 150, 7454, 7458, 4770, 8361, 3257, 1722, 3005, 1597, 2115, 1101, 3534, 593, 48082, 597, 26198, 2005, 6874, 37741, 48877, 2671, 7154, 6006, 3578, 2427, 32124, 2431}\n",
      "Items rated in train (first 20): []\n",
      "Candidate items for prediction (first 20 of 1000): [154260, 111585, 177601, 80574, 100291, 171947, 169004, 122302, 191277, 142652, 70978, 125449, 180695, 160058, 153955, 208293, 5829, 145905, 5802, 5981]\n",
      "Generating raw hybrid predictions for 1000 candidate items...\n",
      "Raw hybrid predictions (Top 30 with individual scores):\n",
      "  MovieID: 124087, PredictedRating: 5.0, Individual: {'svd': 5.0, 'content': 5.0, 'kb': 5.0}\n",
      "  MovieID: 179273, PredictedRating: 5.0, Individual: {'svd': 5.0, 'content': 5.0, 'kb': 5.0}\n",
      "  MovieID: 178247, PredictedRating: 5.0, Individual: {'svd': 5.0, 'content': 5.0, 'kb': 5.0}\n",
      "  MovieID: 156305, PredictedRating: 5.0, Individual: {'svd': 5.0, 'content': 5.0, 'kb': 5.0}\n",
      "  MovieID: 163853, PredictedRating: 5.0, Individual: {'svd': 5.0, 'content': 5.0, 'kb': 5.0}\n",
      "  MovieID: 164765, PredictedRating: 5.0, Individual: {'svd': 5.0, 'content': 5.0, 'kb': 5.0}\n",
      "  MovieID: 156414, PredictedRating: 5.0, Individual: {'svd': 5.0, 'content': 5.0, 'kb': 5.0}\n",
      "  MovieID: 194961, PredictedRating: 5.0, Individual: {'svd': 5.0, 'content': 5.0, 'kb': 5.0}\n",
      "  MovieID: 165787, PredictedRating: 5.0, Individual: {'svd': 5.0, 'content': 5.0, 'kb': 5.0}\n",
      "  MovieID: 139473, PredictedRating: 5.0, Individual: {'svd': 5.0, 'content': 5.0, 'kb': 5.0}\n",
      "  MovieID: 187049, PredictedRating: 4.75, Individual: {'svd': 4.75, 'content': 4.75, 'kb': 4.75}\n",
      "  MovieID: 197371, PredictedRating: 4.75, Individual: {'svd': 4.75, 'content': 4.75, 'kb': 4.75}\n",
      "  MovieID: 144866, PredictedRating: 4.5, Individual: {'svd': 4.5, 'content': 4.5, 'kb': 4.5}\n",
      "  MovieID: 195025, PredictedRating: 4.5, Individual: {'svd': 4.5, 'content': 4.5, 'kb': 4.5}\n",
      "  MovieID: 122371, PredictedRating: 4.5, Individual: {'svd': 4.5, 'content': 4.5, 'kb': 4.5}\n",
      "  MovieID: 198263, PredictedRating: 4.5, Individual: {'svd': 4.5, 'content': 4.5, 'kb': 4.5}\n",
      "  MovieID: 197589, PredictedRating: 4.5, Individual: {'svd': 4.5, 'content': 4.5, 'kb': 4.5}\n",
      "  MovieID: 166496, PredictedRating: 4.5, Individual: {'svd': 4.5, 'content': 4.5, 'kb': 4.5}\n",
      "  MovieID: 4919, PredictedRating: 4.486766746164484, Individual: {'svd': 4.486766746164484, 'content': 4.486766746164484, 'kb': 4.486766746164484}\n",
      "  MovieID: 98335, PredictedRating: 4.25, Individual: {'svd': 4.25, 'content': 4.25, 'kb': 4.25}\n",
      "  MovieID: 136574, PredictedRating: 4.25, Individual: {'svd': 4.25, 'content': 4.25, 'kb': 4.25}\n",
      "  MovieID: 158944, PredictedRating: 4.25, Individual: {'svd': 4.25, 'content': 4.25, 'kb': 4.25}\n",
      "  MovieID: 148060, PredictedRating: 4.25, Individual: {'svd': 4.25, 'content': 4.25, 'kb': 4.25}\n",
      "  MovieID: 171335, PredictedRating: 4.25, Individual: {'svd': 4.25, 'content': 4.25, 'kb': 4.25}\n",
      "  MovieID: 190459, PredictedRating: 4.25, Individual: {'svd': 4.25, 'content': 4.25, 'kb': 4.25}\n",
      "  MovieID: 141737, PredictedRating: 4.25, Individual: {'svd': 4.25, 'content': 4.25, 'kb': 4.25}\n",
      "  MovieID: 58303, PredictedRating: 4.249326599326599, Individual: {'svd': 3.9276094276094278, 'content': 5.0, 'kb': 5.0}\n",
      "  MovieID: 4929, PredictedRating: 4.233924108944496, Individual: {'svd': 4.233924108944496, 'content': 4.233924108944496, 'kb': 4.233924108944496}\n",
      "  MovieID: 103182, PredictedRating: 4.166666666666666, Individual: {'svd': 4.166666666666667, 'content': 4.166666666666667, 'kb': 4.166666666666667}\n",
      "  MovieID: 151651, PredictedRating: 4.166666666666666, Individual: {'svd': 4.166666666666667, 'content': 4.166666666666667, 'kb': 4.166666666666667}\n",
      "Raw hybrid predictions (Bottom 5 with individual scores):\n",
      "  MovieID: 34048, PredictedRating: ERROR: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all(), Individual: {}\n",
      "  MovieID: 178401, PredictedRating: ERROR: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all(), Individual: {}\n",
      "  MovieID: 3553, PredictedRating: ERROR: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all(), Individual: {}\n",
      "  MovieID: 200914, PredictedRating: ERROR: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all(), Individual: {}\n",
      "  MovieID: 192405, PredictedRating: ERROR: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all(), Individual: {}\n",
      "Top-20 recommendations based on get_top_n_recommendations_for_user: [124087, 179273, 178247, 156305, 163853, 164765, 156414, 194961, 165787, 139473, 187049, 197371, 144866, 195025, 122371, 198263, 197589, 166496, 4919, 98335]\n",
      "Intersection with true relevant: set() (Size: 0)\n",
      "--- END DIAGNOSTICS FOR USER: 81933 ---\n",
      "Generating recommendations for user 10/100 (ID: 4524)...\n",
      "Generating recommendations for user 20/100 (ID: 143219)...\n",
      "Generating recommendations for user 30/100 (ID: 19730)...\n",
      "Generating recommendations for user 40/100 (ID: 77053)...\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import time\n",
    "\n",
    "print(\"--- Implementing Ranking Metrics ---\")\n",
    "\n",
    "def get_top_n_recommendations_for_user(user_id, items_to_predict_ids, hybrid_prediction_func, movies_df, ratings_df, algo_svd, trainset_surprise, cosine_sim_content, content_movie_indices_lookup, cosine_sim_genome, genome_movie_indices, genome_movie_ids, hybrid_weights, n=10):\n",
    "    \"\"\"Generates top N recommendations for a single user.\"\"\"\n",
    "    predictions = []\n",
    "    for movie_id in items_to_predict_ids:\n",
    "        try:\n",
    "            # Ensure genome_movie_ids is passed to hybrid_prediction_func\n",
    "            pred_rating, _ = hybrid_prediction_func(\n",
    "                userId=user_id,\n",
    "                movieId=movie_id,\n",
    "                movies_df=movies_df,\n",
    "                ratings_df=ratings_df,\n",
    "                algo_svd=algo_svd,\n",
    "                trainset_surprise=trainset_surprise,\n",
    "                cosine_sim_content=cosine_sim_content,\n",
    "                content_movie_indices_lookup=content_movie_indices_lookup,\n",
    "                cosine_sim_genome=cosine_sim_genome,\n",
    "                genome_movie_indices=genome_movie_indices,\n",
    "                genome_movie_ids=genome_movie_ids, # Restored argument\n",
    "                weights=hybrid_weights\n",
    "            )\n",
    "            predictions.append((movie_id, pred_rating))\n",
    "        except Exception as e:\n",
    "            # print(f\"Error predicting for user {user_id}, movie {movie_id} in top_n: {e}\")\n",
    "            continue # Skip if prediction fails for an item\n",
    "    \n",
    "    predictions.sort(key=lambda x: x[1], reverse=True)\n",
    "    top_n_recs = [item_id for item_id, score in predictions[:n]]\n",
    "    return top_n_recs\n",
    "\n",
    "# ... (precision_recall_at_k and ndcg_at_k functions remain unchanged) ...\n",
    "def precision_recall_at_k(predictions_list, true_relevant_items_map, k_values):\n",
    "    \"\"\"Calculates Precision@k and Recall@k for different k.\"\"\"\n",
    "    precisions = {k: [] for k in k_values}\n",
    "    recalls = {k: [] for k in k_values}\n",
    "\n",
    "    for user_id, top_n_recs in predictions_list.items():\n",
    "        true_relevant_set = true_relevant_items_map.get(user_id, set())\n",
    "        if not true_relevant_set: # Skip user if no relevant items in test set\n",
    "            continue\n",
    "\n",
    "        for k_val in k_values:\n",
    "            if k_val == 0: # Avoid division by zero for k=0\n",
    "                precisions[k_val].append(0.0)\n",
    "                recalls[k_val].append(0.0)\n",
    "                continue\n",
    "            \n",
    "            recommended_at_k = top_n_recs[:k_val]\n",
    "            num_relevant_in_top_k = len(set(recommended_at_k) & true_relevant_set)\n",
    "            \n",
    "            precisions[k_val].append(num_relevant_in_top_k / k_val if k_val > 0 else 0)\n",
    "            recalls[k_val].append(num_relevant_in_top_k / len(true_relevant_set) if len(true_relevant_set) > 0 else 0)\n",
    "\n",
    "    avg_precisions = {k: np.mean(precisions[k]) if precisions[k] else 0 for k in k_values}\n",
    "    avg_recalls = {k: np.mean(recalls[k]) if recalls[k] else 0 for k in k_values}\n",
    "    return avg_precisions, avg_recalls\n",
    "\n",
    "def ndcg_at_k(predictions_list, true_relevant_items_map, k_values):\n",
    "    \"\"\"Calculates nDCG@k for different k.\"\"\"\n",
    "    ndcgs = {k: [] for k in k_values}\n",
    "\n",
    "    for user_id, top_n_recs in predictions_list.items():\n",
    "        true_relevant_set = true_relevant_items_map.get(user_id, set())\n",
    "        if not true_relevant_set: # Skip user if no relevant items in test set for IDCG calculation\n",
    "            continue\n",
    "            \n",
    "        for k_val in k_values:\n",
    "            if k_val == 0:\n",
    "                ndcgs[k_val].append(0.0)\n",
    "                continue\n",
    "\n",
    "            dcg = 0\n",
    "            recommended_at_k = top_n_recs[:k_val]\n",
    "            for i, item_id in enumerate(recommended_at_k):\n",
    "                if item_id in true_relevant_set:\n",
    "                    dcg += 1 / np.log2(i + 2) # Relevance is 1 if relevant, 0 otherwise. Add 2 because log2(1)=0.\n",
    "            \n",
    "            idcg = 0\n",
    "            # Ideal ranking: all relevant items (up to k_val) are at the top\n",
    "            num_relevant_to_consider_for_idcg = min(k_val, len(true_relevant_set))\n",
    "            for i in range(num_relevant_to_consider_for_idcg):\n",
    "                idcg += 1 / np.log2(i + 2)\n",
    "            \n",
    "            ndcgs[k_val].append(dcg / idcg if idcg > 0 else 0)\n",
    "            \n",
    "    avg_ndcgs = {k: np.mean(ndcgs[k]) if ndcgs[k] else 0 for k in k_values}\n",
    "    return avg_ndcgs\n",
    "# ... (rest of the cell setup code remains unchanged) ...\n",
    "print(\"\\\\n--- Setting up for Ranking Metrics Evaluation ---\")\n",
    "\n",
    "# Ensure all necessary components are loaded and available (similar to Cell 18)\n",
    "required_components_ranking = [\n",
    "    'movies_df', 'ratings_df', 'algo_svd', 'trainset_surprise',\n",
    "    'cosine_sim_content', 'content_movie_indices_lookup',\n",
    "    'cosine_sim_genome', 'genome_movie_indices', 'genome_movie_ids',\n",
    "    'testset_surprise', 'hybrid_prediction' # Ensure the prediction function is available\n",
    "]\n",
    "missing_components_ranking = [comp for comp in required_components_ranking if comp not in globals() or globals()[comp] is None]\n",
    "\n",
    "if missing_components_ranking:\n",
    "    print(f\"Error: Missing necessary components for ranking evaluation: {missing_components_ranking}\")\n",
    "    print(\"Please ensure all previous cells, especially data loading, model training/loading, and hybrid function definition, have been run successfully.\")\n",
    "else:\n",
    "    print(\"All necessary components for ranking evaluation appear to be present.\")\n",
    "    \n",
    "    # Parameters for ranking evaluation\n",
    "    K_VALUES_FOR_RANKING = [5, 10, 20] # k values for P@k, R@k, nDCG@k\n",
    "    RELEVANCE_THRESHOLD = 4.0\n",
    "    NUM_USERS_TO_EVALUATE = 100 # Subset of users for faster evaluation. Set to None for all test users.\n",
    "    NUM_ITEMS_TO_PREDICT_PER_USER = 1000 # Number of candidate items to rank for each user.\n",
    "                                        # Predicting for ALL non-training items can be very slow.\n",
    "\n",
    "    # 1. Identify relevant items for each user in the test set\n",
    "    true_relevant_items_map = defaultdict(set)\n",
    "    test_user_item_pairs = defaultdict(list)\n",
    "    for uid, iid, true_r in testset_surprise:\n",
    "        test_user_item_pairs[uid].append(iid)\n",
    "        if true_r >= RELEVANCE_THRESHOLD:\n",
    "            true_relevant_items_map[uid].add(iid)\n",
    "    \n",
    "    print(f\"Identified {len(true_relevant_items_map)} users with relevant items in the test set (rating >= {RELEVANCE_THRESHOLD}).\")\n",
    "\n",
    "    # 2. Select users for evaluation\n",
    "    users_for_evaluation = list(true_relevant_items_map.keys())\n",
    "    if NUM_USERS_TO_EVALUATE is not None and NUM_USERS_TO_EVALUATE < len(users_for_evaluation):\n",
    "        users_for_evaluation = np.random.choice(users_for_evaluation, size=NUM_USERS_TO_EVALUATE, replace=False).tolist()\n",
    "    print(f\"Will evaluate ranking metrics for {len(users_for_evaluation)} users.\")\n",
    "\n",
    "    # 3. Get all unique movie IDs from movies_df to serve as candidate items\n",
    "    all_movie_ids_global = movies_df['movieId'].unique()\n",
    "    \n",
    "    # 4. Generate top-N recommendations for selected users\n",
    "    all_users_top_n_recs = {}\n",
    "    evaluation_start_time = time.time()\n",
    "\n",
    "    hybrid_weights_for_ranking = globals().get('hybrid_weights', (0.7, 0.15, 0.15)) \n",
    "    print(f\"Using hybrid_weights for ranking: {hybrid_weights_for_ranking}\")\n",
    "\n",
    "    first_user_diagnostics_done = False \n",
    "\n",
    "    for i, user_id in enumerate(users_for_evaluation):\n",
    "        if (i + 1) % 10 == 0:\n",
    "            if not (not first_user_diagnostics_done and i == 0):\n",
    "                 print(f\"Generating recommendations for user {i+1}/{len(users_for_evaluation)} (ID: {user_id})...\")\n",
    "        \n",
    "        try:\n",
    "            inner_uid_diag = trainset_surprise.to_inner_uid(user_id)\n",
    "            items_rated_by_user_in_train = {trainset_surprise.to_raw_iid(iid) for iid in trainset_surprise.ur[inner_uid_diag]}\n",
    "        except ValueError: \n",
    "            items_rated_by_user_in_train = set()\n",
    "        except Exception as e_train_items: \n",
    "            items_rated_by_user_in_train = set()\n",
    "\n",
    "        candidate_items_for_user = list(set(all_movie_ids_global) - items_rated_by_user_in_train)\n",
    "        \n",
    "        if NUM_ITEMS_TO_PREDICT_PER_USER is not None and NUM_ITEMS_TO_PREDICT_PER_USER < len(candidate_items_for_user):\n",
    "            items_to_predict_for_this_user = np.random.choice(candidate_items_for_user, size=NUM_ITEMS_TO_PREDICT_PER_USER, replace=False).tolist()\n",
    "        else:\n",
    "            items_to_predict_for_this_user = candidate_items_for_user\n",
    "        \n",
    "        if not items_to_predict_for_this_user:\n",
    "            all_users_top_n_recs[user_id] = []\n",
    "            if not first_user_diagnostics_done: \n",
    "                print(f\"\\\\n--- DIAGNOSTICS FOR FIRST USER: {user_id} ---\")\n",
    "                print(f\"User {user_id} has no candidate items to predict. Skipping actual prediction.\")\n",
    "                print(f\"True relevant items: {true_relevant_items_map.get(user_id, set())}\")\n",
    "                print(f\"Items rated in train: {items_rated_by_user_in_train}\")\n",
    "                print(f\"--- END DIAGNOSTICS FOR USER: {user_id} ---\")\n",
    "                first_user_diagnostics_done = True\n",
    "            continue\n",
    "\n",
    "        max_k_needed = max(K_VALUES_FOR_RANKING) if K_VALUES_FOR_RANKING else 20 \n",
    "\n",
    "        if not first_user_diagnostics_done:\n",
    "            print(f\"\\\\n--- DIAGNOSTICS FOR FIRST USER: {user_id} ---\")\n",
    "            print(f\"Processing user {i+1}/{len(users_for_evaluation)} (ID: {user_id}) for diagnostics.\")\n",
    "            print(f\"True relevant items: {true_relevant_items_map.get(user_id, set())}\")\n",
    "            print(f\"Items rated in train (first 20): {list(items_rated_by_user_in_train)[:20]}\")\n",
    "            print(f\"Candidate items for prediction (first 20 of {len(items_to_predict_for_this_user)}): {items_to_predict_for_this_user[:20]}\")\n",
    "\n",
    "            raw_predictions_for_diag_user = []\n",
    "            print(f\"Generating raw hybrid predictions for {len(items_to_predict_for_this_user)} candidate items...\")\n",
    "            for item_idx, movie_id_diag in enumerate(items_to_predict_for_this_user):\n",
    "                if item_idx < 30 or item_idx % 100 == 0 : \n",
    "                    pass\n",
    "                try:\n",
    "                    # Ensure genome_movie_ids is passed here as well for diagnostics\n",
    "                    pred_rating_diag, individual_scores_diag = hybrid_prediction(\n",
    "                        userId=user_id, movieId=movie_id_diag,\n",
    "                        movies_df=movies_df, ratings_df=ratings_df, algo_svd=algo_svd,\n",
    "                        trainset_surprise=trainset_surprise, cosine_sim_content=cosine_sim_content,\n",
    "                        content_movie_indices_lookup=content_movie_indices_lookup,\n",
    "                        cosine_sim_genome=cosine_sim_genome, genome_movie_indices=genome_movie_indices,\n",
    "                        genome_movie_ids=genome_movie_ids, # Restored argument\n",
    "                        weights=hybrid_weights_for_ranking\n",
    "                    )\n",
    "                    raw_predictions_for_diag_user.append((movie_id_diag, pred_rating_diag, individual_scores_diag))\n",
    "                except Exception as e_diag:\n",
    "                    raw_predictions_for_diag_user.append((movie_id_diag, f\"ERROR: {str(e_diag)}\", {}))\n",
    "            \n",
    "            raw_predictions_for_diag_user.sort(\n",
    "                key=lambda x: x[1] if isinstance(x[1], (int, float)) else -1.0, \n",
    "                reverse=True\n",
    "            )\n",
    "            print(f\"Raw hybrid predictions (Top 30 with individual scores):\")\n",
    "            for pred_item in raw_predictions_for_diag_user[:30]:\n",
    "                 print(f\"  MovieID: {pred_item[0]}, PredictedRating: {pred_item[1]}, Individual: {pred_item[2]}\")\n",
    "            if len(raw_predictions_for_diag_user) > 30:\n",
    "                print(f\"Raw hybrid predictions (Bottom 5 with individual scores):\")\n",
    "                for pred_item in raw_predictions_for_diag_user[-5:]:\n",
    "                    print(f\"  MovieID: {pred_item[0]}, PredictedRating: {pred_item[1]}, Individual: {pred_item[2]}\")\n",
    "\n",
    "        # Actual call to get top N for all users\n",
    "        # Ensure genome_movie_ids is passed here\n",
    "        user_top_n = get_top_n_recommendations_for_user(\n",
    "            user_id, items_to_predict_for_this_user, hybrid_prediction, \n",
    "            movies_df, ratings_df, algo_svd, trainset_surprise, \n",
    "            cosine_sim_content, content_movie_indices_lookup, \n",
    "            cosine_sim_genome, genome_movie_indices, genome_movie_ids, # Restored argument\n",
    "            hybrid_weights_for_ranking, n=max_k_needed\n",
    "        )\n",
    "        all_users_top_n_recs[user_id] = user_top_n\n",
    "\n",
    "        if not first_user_diagnostics_done:\n",
    "            print(f\"Top-{max_k_needed} recommendations based on get_top_n_recommendations_for_user: {user_top_n}\")\n",
    "            relevant_for_diag_user = true_relevant_items_map.get(user_id, set())\n",
    "            recommended_set = set(user_top_n)\n",
    "            intersection = relevant_for_diag_user.intersection(recommended_set)\n",
    "            print(f\"Intersection with true relevant: {intersection} (Size: {len(intersection)})\")\n",
    "            print(f\"--- END DIAGNOSTICS FOR USER: {user_id} ---\")\n",
    "            first_user_diagnostics_done = True\n",
    "            # if i == 0: \n",
    "            #     print(\"Diagnostic run complete for the first user. Breaking loop.\")\n",
    "            #     break \n",
    "# ... (The rest of the cell, including calculation and printing of metrics, remains unchanged) ...\n",
    "# This part is omitted for brevity but should be included in the actual edit if it was part of the original cell.\n",
    "# Assuming the cell ends after the loop for users_for_evaluation or proceeds to calculate and print metrics.\n",
    "# For this edit, we are only focusing on restoring genome_movie_ids.\n",
    "# If the cell had more code after the loop, it should be preserved."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
