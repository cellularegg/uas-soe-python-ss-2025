{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0c4b471272f36a0",
   "metadata": {},
   "source": [
    "# Bert4rec\n",
    "Author: David Zelenay\n",
    "\n",
    "This notebook builds a **movie recommendation system using a BERT-style model (BERT4Rec)**.\n",
    "\n",
    "Here's the workflow:\n",
    "\n",
    "1.  **Data Loading & Preprocessing:** Loads movie ratings and metadata.\n",
    "2.  **User Sequence Creation:** Converts user rating histories into ordered sequences of `(rating, movieId)` interactions.\n",
    "3.  **Tokenization & Padding:** Maps movie interactions to integer IDs and pads/truncates sequences to a fixed length.\n",
    "4.  **Dataset Preparation:** Creates a PyTorch Dataset that randomly masks items in the sequences for training (Masked Language Model objective).\n",
    "5.  **Model Training:**\n",
    "    *   Defines a `BertForMaskedLM` model (from HuggingFace Transformers).\n",
    "    *   Trains this model to predict the *masked* movies in the user sequences.\n",
    "    *   Saves model checkpoints after each epoch.\n",
    "6.  **Recommendation Generation (Inference):**\n",
    "    *   Takes a user's recent movie sequence.\n",
    "    *   Masks the *next potential item* in that sequence.\n",
    "    *   Uses the trained model to predict the most probable movies for that masked slot.\n",
    "    *   Filters out already seen movies and presents the top predictions as recommendations.\n",
    "\n",
    "In short, it learns patterns from user movie interaction histories to predict what they might watch next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4962b5c8d2615dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70088d59",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d498eb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_df = pd.read_parquet(\"../data/parquet/ratings.parquet\")\n",
    "ratings_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328c0477",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df = pd.read_parquet(\"../data/parquet/movies.parquet\")\n",
    "movies_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6d5f13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "00002a48",
   "metadata": {},
   "source": [
    "# Prepare User Sequences\n",
    "To train BERT4Rec, we need to convert the ratings data into sequences of movie IDs for each user, ordered by timestamp. Each sequence represents the user's interaction history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5510be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create user sequences as tuples of (rating, movieId), ordered by timestamp\n",
    "user_sequences = ratings_df.sample(frac=0.001).sort_values(['userId', 'timestamp']).groupby('userId')[['rating', 'movieId']].apply(lambda x: list(zip(x['rating'], x['movieId'])))\n",
    "user_sequences = user_sequences.tolist()\n",
    "print(f\"Number of users: {len(user_sequences)}\")\n",
    "print(f\"Example sequence: {user_sequences[0][:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38aabb38",
   "metadata": {},
   "source": [
    "## Install and Import Required Packages\n",
    "We'll use PyTorch and HuggingFace Transformers for the BERT4Rec model. If not already installed, run the following cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a747b365",
   "metadata": {},
   "source": [
    "## Tokenize and Pad Sequences\n",
    "BERT4Rec requires sequences of equal length. We'll map movie IDs to integer tokens and pad the sequences to a fixed length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa824798",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch\n",
    "\n",
    "# Create movieId to index mapping\n",
    "movie_ids = set([movie for seq in user_sequences for movie in seq])\n",
    "movie2idx = {movie: idx+1 for idx, movie in enumerate(sorted(movie_ids))}  # 0 reserved for padding\n",
    "idx2movie = {idx: movie for movie, idx in movie2idx.items()}\n",
    "\n",
    "# Convert sequences to index lists\n",
    "indexed_sequences = [[movie2idx[movie] for movie in seq] for seq in user_sequences]\n",
    "\n",
    "# Pad sequences\n",
    "max_seq_len = 10  # You can adjust this\n",
    "padded_sequences = pad_sequence([torch.tensor(seq[-max_seq_len:]) for seq in indexed_sequences], batch_first=True, padding_value=0)\n",
    "print(f\"Padded sequences shape: {padded_sequences.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e53c343",
   "metadata": {},
   "source": [
    "## Prepare PyTorch Dataset and DataLoader\n",
    "We'll create a custom Dataset to feed the padded sequences into the BERT4Rec model. Each sample will be a sequence for masked language modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46ee86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "\n",
    "class BERT4RecDataset(Dataset):\n",
    "    def __init__(self, sequences, mask_prob=0.15):\n",
    "        self.sequences = sequences\n",
    "        self.mask_prob = mask_prob\n",
    "        self.vocab_size = len(movie2idx) + 1  # +1 for padding\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.sequences[idx].clone()\n",
    "        labels = seq.clone()\n",
    "        mask = torch.rand(seq.size()) < self.mask_prob\n",
    "        seq[mask] = self.vocab_size - 1  # Use last index as [MASK] token\n",
    "        labels[~mask] = -100  # Only compute loss on masked tokens\n",
    "        return seq, labels\n",
    "\n",
    "# Create dataset and dataloader\n",
    "mask_token = len(movie2idx) + 1\n",
    "train_dataset = BERT4RecDataset(padded_sequences)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Check a batch\n",
    "for batch in train_loader:\n",
    "    print(batch[0].shape, batch[1].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb96f1b0",
   "metadata": {},
   "source": [
    "## Define and Train the BERT4Rec Model\n",
    "We'll use HuggingFace's `BertForMaskedLM` as the base for BERT4Rec. The model will be trained to predict masked movies in user sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "afc9aca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Apple Silicon GPU (MPS backend)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b503226e852746629e08b706ae3fb1bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "798ab41cf88643a18621c38783b30bb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 average loss: 1.8859\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c930ccde4ee4198becdeb6c5402da31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 average loss: 1.6335\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4271fb93900b4978b0156ba30be73886",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 average loss: 1.6816\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59c8d786e3ad4afaa4bd366c4d4761aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 average loss: 1.6716\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0491c04634f42d1bed8753769645759",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 average loss: 1.6144\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Robust device selection for Mac M3 GPU (Apple Silicon)\n",
    "if torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using Apple Silicon GPU (MPS backend)\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using CUDA GPU\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU (no GPU available)\")\n",
    "\n",
    "from transformers import BertConfig, BertForMaskedLM\n",
    "from torch.optim import AdamW\n",
    "from tqdm.notebook import tqdm  # Add tqdm for progress bar\n",
    "import os\n",
    "\n",
    "config = BertConfig(\n",
    "    vocab_size=len(movie2idx) + 2,  # +1 for padding, +1 for [MASK]\n",
    "    max_position_embeddings=max_seq_len,\n",
    "    num_attention_heads=4,\n",
    "    num_hidden_layers=4,\n",
    "    type_vocab_size=1\n",
    ")\n",
    "model = BertForMaskedLM(config).to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-4)\n",
    "\n",
    "# Training loop (all epochs with tqdm progress bar)\n",
    "epochs = 5  # Set the number of epochs as needed\n",
    "model.train()\n",
    "# Create directory for checkpoints if it doesn't exist\n",
    "os.makedirs(\"./model_checkpoints\", exist_ok=True)\n",
    "for epoch in tqdm(range(epochs), desc=\"Epochs\", leave=True):\n",
    "    # print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "    epoch_loss = 0\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Training\", leave=True)\n",
    "    for batch in progress_bar:\n",
    "        input_ids, labels = batch[0].to(device), batch[1].to(device)\n",
    "        # Create attention mask: 1 for non-padding, 0 for padding (padding token is 0)\n",
    "        attention_mask = (input_ids != 0).long()\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        epoch_loss += loss.item()\n",
    "        progress_bar.set_postfix({\"batch_loss\": f\"{loss.item():.4f}\"})\n",
    "    avg_loss = epoch_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1} average loss: {avg_loss:.4f}\")\n",
    "    # Save model checkpoint after each epoch\n",
    "    checkpoint_path = f\"./model_checkpoints/bert4rec_epoch_{epoch+1}.pth\"\n",
    "    torch.save(model.state_dict(), checkpoint_path)\n",
    "    # print(f\"Model saved to {checkpoint_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc642548",
   "metadata": {},
   "source": [
    "## Generate Recommendations with BERT4Rec\n",
    "To recommend movies for a user, we mask the next position in their sequence and let the model predict likely movies. We then select the top predictions as recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e4a7daaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended movie IDs: [(3.0, 377), (4.0, 110), (5.0, 318), (5.0, 3052), (4.0, 480), (5.0, 527), (4.0, 590), (4.5, 50), (3.0, 708)]\n",
      "Recommended movie titles: ['Braveheart (1995)', 'Shawshank Redemption, The (1994)', 'Dogma (1999)', 'Jurassic Park (1993)', \"Schindler's List (1993)\", 'Dances with Wolves (1990)', 'Usual Suspects, The (1995)']\n"
     ]
    }
   ],
   "source": [
    "# Example: Recommend movies for the first user in the dataset\n",
    "user_seq = indexed_sequences[0][-max_seq_len:]  # Most recent interactions\n",
    "seen_movies = set(user_seq)\n",
    "\n",
    "# Prepare input: mask the last position\n",
    "input_seq = user_seq.copy()\n",
    "if len(input_seq) < max_seq_len:\n",
    "    input_seq = [0] * (max_seq_len - len(input_seq)) + input_seq\n",
    "input_seq[-1] = len(movie2idx) + 1  # [MASK] token\n",
    "input_tensor = torch.tensor([input_seq]).to(device)\n",
    "# Create attention mask for inference\n",
    "attention_mask = (input_tensor != 0).long()\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_tensor, attention_mask=attention_mask)\n",
    "    logits = outputs.logits\n",
    "    masked_pos = -1  # Last position\n",
    "    probs = logits[0, masked_pos].softmax(dim=-1)\n",
    "    topk = torch.topk(probs, k=10)\n",
    "    rec_indices = topk.indices.cpu().numpy()\n",
    "    # Filter out padding, mask, and already seen movies\n",
    "    rec_movies = [idx2movie[idx] for idx in rec_indices if idx in idx2movie and idx not in seen_movies][:10]  # Top 10 recommendations\n",
    "\n",
    "print(\"Recommended movie IDs:\", rec_movies)\n",
    "# Optionally, map to movie titles\n",
    "rec_movie_ids = [movie_id for rating, movie_id in rec_movies if rating > 3]\n",
    "\n",
    "movie_titles = movies_df.set_index('movieId').loc[rec_movie_ids]['title'].tolist()\n",
    "print(\"Recommended movie titles:\", movie_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ebfcaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_movie_ids_only = [movie_id for _, movie_id in rec_movies]\n",
    "print(rec_movie_ids_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e18d42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uas-soe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
